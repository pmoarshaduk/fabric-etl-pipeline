{"cells":[{"cell_type":"code","source":["import pyspark.sql.functions as F\n","from pyspark.sql.types import *\n","import time\n","import builtins\n","\n","# ==============================================================================\n","# STEP 1: CONFIGURATION\n","# ==============================================================================\n","ROWS_TO_GENERATE = 1000000 \n","TARGET_TABLE = \"Lake24.dbo.person\"\n","\n","print(f\"ðŸŽ¬ Starting generation of {ROWS_TO_GENERATE:,} records...\")\n","\n","# ==============================================================================\n","# STEP 2: LIVE METADATA ANALYSIS (DYNAMIC)\n","# ==============================================================================\n","# Load the target table once to get both schema and current stats\n","target_table_df = spark.table(TARGET_TABLE)\n","actual_schema = target_table_df.schema\n","\n","# Get the LIVE count and MAX ID\n","stats_df = target_table_df.select(\n","    F.count(\"*\").alias(\"current_count\"), \n","    F.max(\"person_id\").alias(\"max_id\")\n",").collect()[0]\n","\n","initial_row_count = stats_df[\"current_count\"]\n","max_id_val = stats_df[\"max_id\"]\n","start_id = (max_id_val or 0) + 1\n","\n","print(f\"ðŸ” Table Analysis:\")\n","print(f\"   - Existing Records: {initial_row_count:,}\")\n","print(f\"   - Current Max ID:   {max_id_val:,}\")\n","print(f\"   - Resuming from:    {start_id:,}\")\n","\n","# ==============================================================================\n","# STEP 3: VECTORIZED SYNTHETIC GENERATION\n","# ==============================================================================\n","print(f\"âš¡ Generating {ROWS_TO_GENERATE:,} rows in-memory...\")\n","start_time = time.time()\n","\n","df_synth = spark.range(0, ROWS_TO_GENERATE) \\\n","    .withColumn(\"person_id\", (F.lit(start_id) + F.col(\"id\")).cast(IntegerType())) \\\n","    .withColumn(\"gender_rand\", F.rand()) \\\n","    .withColumn(\"gender_concept_id\", F.when(F.col(\"gender_rand\") > 0.52, 8532).otherwise(8507).cast(IntegerType())) \\\n","    .withColumn(\"year_of_birth\", (F.rand() * 70 + 1950).cast(IntegerType())) \\\n","    .withColumn(\"month_of_birth\", (F.rand() * 11 + 1).cast(IntegerType())) \\\n","    .withColumn(\"day_of_birth\", (F.rand() * 27 + 1).cast(IntegerType())) \\\n","    .withColumn(\"race_rand\", F.rand()) \\\n","    .withColumn(\"race_concept_id\", \n","        F.when(F.col(\"race_rand\") < 0.60, 8527)\n","         .when(F.col(\"race_rand\") < 0.74, 8515)\n","         .otherwise(0).cast(IntegerType())) \\\n","    .withColumn(\"ethnicity_concept_id\", F.lit(38003564).cast(IntegerType())) \\\n","    .withColumn(\"person_source_value\", F.concat(F.lit(\"SYNTH_\"), F.col(\"person_id\").cast(\"string\"))) \\\n","    .withColumn(\"gender_source_value\", F.when(F.col(\"gender_concept_id\") == 8507, \"M\").otherwise(\"F\")) \\\n","    .withColumn(\"gender_source_concept_id\", F.col(\"gender_concept_id\").cast(IntegerType())) \\\n","    .withColumn(\"birth_datetime\", F.to_date(\n","        F.concat_ws(\"-\", F.col(\"year_of_birth\"), F.col(\"month_of_birth\"), F.col(\"day_of_birth\"))\n","    ))\n","\n","# Align columns based on the actual schema profile\n","final_columns = []\n","for field in actual_schema.fields:\n","    if field.name in df_synth.columns:\n","        final_columns.append(F.col(field.name).cast(field.dataType))\n","    else:\n","        final_columns.append(F.lit(None).cast(field.dataType).alias(field.name))\n","\n","final_df = df_synth.select(final_columns)\n","\n","# ==============================================================================\n","# STEP 4: APPEND TO DELTA TABLE\n","# ==============================================================================\n","try:\n","    final_df.write.format(\"delta\").mode(\"append\").saveAsTable(TARGET_TABLE)\n","    execution_time = builtins.round(time.time() - start_time, 2)\n","    print(f\"âœ… SUCCESS: Processed in {execution_time} seconds.\")\n","except Exception as e:\n","    print(f\"âŒ Error: {str(e)}\")\n","\n","# ==============================================================================\n","# STEP 5: FINAL REPORT (DYNAMIC SUMMARY)\n","# ==============================================================================\n","final_count = spark.table(TARGET_TABLE).count()\n","\n","print(\"\\n\" + \"=\"*40)\n","print(\"ðŸ“Š SYNTHETIC DATA GENERATION SUMMARY\")\n","print(\"=\"*40)\n","print(f\"Target Table:     {TARGET_TABLE}\")\n","print(f\"Initial Rows:     {initial_row_count:,}\")\n","print(f\"Added Rows:       {ROWS_TO_GENERATE:,}\")\n","print(f\"Final Row Count:  {final_count:,}\")\n","print(f\"Execution Speed:  {builtins.round(ROWS_TO_GENERATE/execution_time, 0):,} rows/sec\")\n","print(\"=\"*40)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","spark_jobs_updating":false,"session_id":"35da6aa5-e6a2-4bd8-9766-a4c53fe575f1","normalized_state":"finished","queued_time":"2026-02-28T07:23:01.9006501Z","session_start_time":null,"execution_start_time":"2026-02-28T07:23:01.9017689Z","execution_finish_time":"2026-02-28T07:23:10.3238134Z","parent_msg_id":"956df9f9-7121-493c-ac69-5933c5d91210"},"text/plain":"StatementMeta(, 35da6aa5-e6a2-4bd8-9766-a4c53fe575f1, 12, Finished, Available, Finished, False)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ðŸŽ¬ Starting generation of 1,000,000 records...\nðŸ” Table Analysis:\n   - Existing Records: 14,712,818\n   - Current Max ID:   311,960,608\n   - Resuming from:    311,960,609\nâš¡ Generating 1,000,000 rows in-memory...\nâœ… SUCCESS: Processed in 2.58 seconds.\n\n========================================\nðŸ“Š SYNTHETIC DATA GENERATION SUMMARY\n========================================\nTarget Table:     Lake24.dbo.person\nInitial Rows:     14,712,818\nAdded Rows:       1,000,000\nFinal Row Count:  15,712,818\nExecution Speed:  387,597.0 rows/sec\n========================================\n"]}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0466933f-9029-4ab5-8ddd-4c357300fc1f"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"5d277b6d-26ce-4249-a7d9-a6eac8face79"}],"default_lakehouse":"5d277b6d-26ce-4249-a7d9-a6eac8face79","default_lakehouse_name":"Lake24","default_lakehouse_workspace_id":"591f29e6-d45a-4989-9459-a5a7bf1b39b8"}}},"nbformat":4,"nbformat_minor":5}