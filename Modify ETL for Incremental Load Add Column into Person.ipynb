{"cells":[{"cell_type":"code","source":["# Databricks notebook source\n","# MAGIC %md\n","# MAGIC # PHASE 1 - STEP 1: ADD AUDIT COLUMNS TO PERSON TABLE\n","# MAGIC \n","# MAGIC **Objective:** Add watermark columns for incremental load pattern\n","# MAGIC \n","# MAGIC **Approach:** Spark native (no SQL ALTER TABLE - Fabric compatible)\n","# MAGIC \n","# MAGIC **Safety:**\n","# MAGIC - ‚úÖ Non-destructive: Reads existing data, adds columns, rewrites\n","# MAGIC - ‚úÖ DAMA compliant: Preserves all 15.7M rows\n","# MAGIC - ‚úÖ Audit trail: Logs all operations\n","# MAGIC - ‚úÖ Rollback: Original table backed up before change\n","# MAGIC \n","# MAGIC **Estimated Time:** 3-5 minutes for 15.7M rows\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## CONFIGURATION\n","\n","# COMMAND ----------\n","\n","import pyspark.sql.functions as F\n","from pyspark.sql.types import *\n","from datetime import datetime\n","import time\n","\n","# Configuration\n","SOURCE_TABLE = \"Lake24.dbo.person\"\n","BACKUP_TABLE = \"Lake24.dbo.person_backup_phase1\"  # Safety backup\n","TEMP_TABLE = \"Lake24.dbo.person_temp_phase1\"       # Temporary work table\n","\n","print(\"=\" * 80)\n","print(\"PHASE 1 - STEP 1: ADD AUDIT COLUMNS\")\n","print(\"=\" * 80)\n","print(f\"Source Table: {SOURCE_TABLE}\")\n","print(f\"Backup Table: {BACKUP_TABLE}\")\n","print(f\"Spark Version: {spark.version}\")\n","print(\"=\" * 80)\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## PRE-FLIGHT CHECKS\n","\n","# COMMAND ----------\n","\n","def preflight_checks():\n","    \"\"\"\n","    Safety checks before modifying schema\n","    \n","    Returns: (success: bool, current_count: int, current_schema: list)\n","    \"\"\"\n","    print(\"\\nüîç PRE-FLIGHT CHECKS:\")\n","    print(\"-\" * 80)\n","    \n","    try:\n","        # 1. Verify source table exists\n","        if not spark.catalog.tableExists(SOURCE_TABLE):\n","            print(f\"‚ùå ERROR: Table {SOURCE_TABLE} does not exist!\")\n","            return False, 0, []\n","        print(f\"‚úÖ Source table exists: {SOURCE_TABLE}\")\n","        \n","        # 2. Get current record count\n","        source_df = spark.table(SOURCE_TABLE)\n","        current_count = source_df.count()\n","        print(f\"‚úÖ Current record count: {current_count:,}\")\n","        \n","        # 3. Get current schema\n","        current_schema = source_df.schema.fields\n","        current_columns = [f.name for f in current_schema]\n","        print(f\"‚úÖ Current column count: {len(current_columns)}\")\n","        \n","        # 4. Check if audit columns already exist\n","        audit_columns = ['created_timestamp', 'updated_timestamp', 'is_deleted']\n","        existing_audit = [col for col in audit_columns if col in current_columns]\n","        \n","        if existing_audit:\n","            print(f\"‚ö†Ô∏è  WARNING: Some audit columns already exist: {existing_audit}\")\n","            print(f\"   This script will preserve existing values.\")\n","        else:\n","            print(f\"‚úÖ No audit columns exist yet (will add: {audit_columns})\")\n","        \n","        # 5. Check if backup already exists\n","        if spark.catalog.tableExists(BACKUP_TABLE):\n","            print(f\"‚ö†Ô∏è  WARNING: Backup table already exists: {BACKUP_TABLE}\")\n","            print(f\"   Will be overwritten with current data.\")\n","        \n","        # 6. Estimate processing time\n","        estimated_time = current_count / 100000  # ~100K rows/second estimate\n","        print(f\"üìä Estimated processing time: {estimated_time:.1f} seconds ({estimated_time/60:.1f} minutes)\")\n","        \n","        print(\"-\" * 80)\n","        print(\"‚úÖ ALL PRE-FLIGHT CHECKS PASSED\")\n","        print(\"=\" * 80)\n","        \n","        return True, current_count, current_columns\n","        \n","    except Exception as e:\n","        print(f\"‚ùå PRE-FLIGHT CHECK FAILED: {str(e)}\")\n","        return False, 0, []\n","\n","# Run pre-flight checks\n","checks_passed, record_count, existing_columns = preflight_checks()\n","\n","if not checks_passed:\n","    raise Exception(\"Pre-flight checks failed. Aborting.\")\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## STEP 1.1: CREATE BACKUP (SAFETY)\n","\n","# COMMAND ----------\n","\n","def create_backup():\n","    \"\"\"\n","    Create backup of original table before modification\n","    \n","    DAMA Best Practice: Always backup before schema changes\n","    \"\"\"\n","    print(\"\\nüì¶ CREATING BACKUP:\")\n","    print(\"-\" * 80)\n","    \n","    start_time = time.time()\n","    \n","    try:\n","        # Drop backup if exists (overwrite with fresh backup)\n","        if spark.catalog.tableExists(BACKUP_TABLE):\n","            print(f\"   Dropping existing backup: {BACKUP_TABLE}\")\n","            spark.sql(f\"DROP TABLE IF EXISTS {BACKUP_TABLE}\")\n","        \n","        # Create backup (exact copy)\n","        print(f\"   Creating backup: {BACKUP_TABLE}\")\n","        source_df = spark.table(SOURCE_TABLE)\n","        \n","        source_df.write \\\n","            .format(\"delta\") \\\n","            .mode(\"overwrite\") \\\n","            .saveAsTable(BACKUP_TABLE)\n","        \n","        # Verify backup\n","        backup_count = spark.table(BACKUP_TABLE).count()\n","        \n","        duration = time.time() - start_time\n","        print(f\"‚úÖ Backup created successfully\")\n","        print(f\"   Records backed up: {backup_count:,}\")\n","        print(f\"   Duration: {duration:.2f} seconds\")\n","        print(f\"   Location: {BACKUP_TABLE}\")\n","        print(\"-\" * 80)\n","        \n","        return True\n","        \n","    except Exception as e:\n","        print(f\"‚ùå BACKUP FAILED: {str(e)}\")\n","        print(f\"   ABORTING: Cannot proceed without backup\")\n","        return False\n","\n","# Create backup\n","backup_success = create_backup()\n","\n","if not backup_success:\n","    raise Exception(\"Backup creation failed. Aborting for safety.\")\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## STEP 1.2: ADD AUDIT COLUMNS\n","\n","# COMMAND ----------\n","\n","def add_audit_columns():\n","    \"\"\"\n","    Add audit columns to person table using Spark native approach\n","    \n","    Method:\n","    1. Read existing table\n","    2. Add new columns with appropriate defaults\n","    3. Write to temporary table\n","    4. Swap tables (atomic operation)\n","    \n","    Columns Added:\n","    - created_timestamp: When record was first created\n","    - updated_timestamp: When record was last modified\n","    - is_deleted: Soft delete flag\n","    \n","    Default Values:\n","    - Existing records: NULL for timestamps (unknown), False for is_deleted\n","    - Future records: Will be populated by source system or ETL\n","    \"\"\"\n","    print(\"\\nüîß ADDING AUDIT COLUMNS:\")\n","    print(\"-\" * 80)\n","    \n","    start_time = time.time()\n","    \n","    try:\n","        # 1. Read existing table\n","        print(\"   Step 1: Reading source table...\")\n","        source_df = spark.table(SOURCE_TABLE)\n","        original_count = source_df.count()\n","        print(f\"   ‚úÖ Read {original_count:,} records\")\n","        \n","        # 2. Add audit columns\n","        print(\"   Step 2: Adding audit columns...\")\n","        \n","        # Check which columns already exist\n","        existing_cols = source_df.columns\n","        \n","        # Add created_timestamp (if not exists)\n","        if 'created_timestamp' not in existing_cols:\n","            source_df = source_df.withColumn(\n","                \"created_timestamp\", \n","                F.lit(None).cast(TimestampType())\n","            )\n","            print(\"   ‚úÖ Added: created_timestamp (TIMESTAMP, NULL for existing)\")\n","        else:\n","            print(\"   ‚ö†Ô∏è  Skipped: created_timestamp (already exists)\")\n","        \n","        # Add updated_timestamp (if not exists)\n","        if 'updated_timestamp' not in existing_cols:\n","            source_df = source_df.withColumn(\n","                \"updated_timestamp\", \n","                F.lit(None).cast(TimestampType())\n","            )\n","            print(\"   ‚úÖ Added: updated_timestamp (TIMESTAMP, NULL for existing)\")\n","        else:\n","            print(\"   ‚ö†Ô∏è  Skipped: updated_timestamp (already exists)\")\n","        \n","        # Add is_deleted (if not exists)\n","        if 'is_deleted' not in existing_cols:\n","            source_df = source_df.withColumn(\n","                \"is_deleted\", \n","                F.lit(False).cast(BooleanType())\n","            )\n","            print(\"   ‚úÖ Added: is_deleted (BOOLEAN, False for existing)\")\n","        else:\n","            print(\"   ‚ö†Ô∏è  Skipped: is_deleted (already exists)\")\n","        \n","        # 3. Verify schema\n","        new_schema = source_df.schema.fields\n","        new_columns = [f.name for f in new_schema]\n","        print(f\"   ‚úÖ New schema has {len(new_columns)} columns (was {len(existing_columns)})\")\n","        \n","        # 4. Write to temporary table\n","        print(\"   Step 3: Writing to temporary table...\")\n","        \n","        # Drop temp table if exists\n","        if spark.catalog.tableExists(TEMP_TABLE):\n","            spark.sql(f\"DROP TABLE IF EXISTS {TEMP_TABLE}\")\n","        \n","        source_df.write \\\n","            .format(\"delta\") \\\n","            .mode(\"overwrite\") \\\n","            .option(\"overwriteSchema\", \"true\") \\\n","            .saveAsTable(TEMP_TABLE)\n","        \n","        # Verify temp table\n","        temp_count = spark.table(TEMP_TABLE).count()\n","        print(f\"   ‚úÖ Wrote {temp_count:,} records to temp table\")\n","        \n","        # 5. Verify record count matches\n","        if temp_count != original_count:\n","            raise Exception(f\"Record count mismatch! Original: {original_count:,}, Temp: {temp_count:,}\")\n","        print(f\"   ‚úÖ Record count verified: {temp_count:,} = {original_count:,}\")\n","        \n","        duration = time.time() - start_time\n","        print(f\"‚úÖ Audit columns added successfully\")\n","        print(f\"   Duration: {duration:.2f} seconds\")\n","        print(\"-\" * 80)\n","        \n","        return True, temp_count\n","        \n","    except Exception as e:\n","        print(f\"‚ùå ADDING COLUMNS FAILED: {str(e)}\")\n","        print(f\"   Rolling back...\")\n","        \n","        # Cleanup temp table\n","        try:\n","            if spark.catalog.tableExists(TEMP_TABLE):\n","                spark.sql(f\"DROP TABLE IF EXISTS {TEMP_TABLE}\")\n","                print(\"   ‚úÖ Cleaned up temporary table\")\n","        except:\n","            pass\n","        \n","        return False, 0\n","\n","# Add audit columns\n","add_success, new_count = add_audit_columns()\n","\n","if not add_success:\n","    print(\"\\nüîÑ ROLLBACK: Restoring from backup...\")\n","    # Source table unchanged, backup exists\n","    print(\"‚úÖ Original table unchanged. Backup available at:\", BACKUP_TABLE)\n","    raise Exception(\"Adding columns failed. Original table is safe.\")\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## STEP 1.3: ATOMIC TABLE SWAP\n","\n","# COMMAND ----------\n","\n","def swap_tables():\n","    \"\"\"\n","    Atomically swap temp table with source table\n","    \n","    Strategy:\n","    1. Rename source ‚Üí source_old\n","    2. Rename temp ‚Üí source\n","    3. Drop source_old\n","    \n","    This ensures minimal downtime and atomic switch\n","    \"\"\"\n","    print(\"\\nüîÑ ATOMIC TABLE SWAP:\")\n","    print(\"-\" * 80)\n","    \n","    try:\n","        # 1. Rename source to old\n","        print(f\"   Step 1: Renaming {SOURCE_TABLE} ‚Üí person_old...\")\n","        spark.sql(f\"ALTER TABLE {SOURCE_TABLE} RENAME TO Lake24.dbo.person_old\")\n","        print(\"   ‚úÖ Source renamed to person_old\")\n","        \n","        # 2. Rename temp to source\n","        print(f\"   Step 2: Renaming {TEMP_TABLE} ‚Üí {SOURCE_TABLE}...\")\n","        spark.sql(f\"ALTER TABLE {TEMP_TABLE} RENAME TO {SOURCE_TABLE}\")\n","        print(f\"   ‚úÖ Temp renamed to {SOURCE_TABLE}\")\n","        \n","        # 3. Verify new table\n","        new_count = spark.table(SOURCE_TABLE).count()\n","        print(f\"   ‚úÖ Verified: {SOURCE_TABLE} has {new_count:,} records\")\n","        \n","        # 4. Drop old table\n","        print(f\"   Step 3: Dropping person_old...\")\n","        spark.sql(\"DROP TABLE IF EXISTS Lake24.dbo.person_old\")\n","        print(\"   ‚úÖ Old table dropped\")\n","        \n","        print(\"-\" * 80)\n","        print(\"‚úÖ TABLE SWAP COMPLETED SUCCESSFULLY\")\n","        print(\"=\" * 80)\n","        \n","        return True\n","        \n","    except Exception as e:\n","        print(f\"‚ùå TABLE SWAP FAILED: {str(e)}\")\n","        print(f\"   CRITICAL: Manual intervention required!\")\n","        print(f\"   Current state:\")\n","        print(f\"   - Original table may be at: Lake24.dbo.person_old\")\n","        print(f\"   - Temp table may be at: {TEMP_TABLE}\")\n","        print(f\"   - Backup available at: {BACKUP_TABLE}\")\n","        return False\n","\n","# Perform atomic swap\n","swap_success = swap_tables()\n","\n","if not swap_success:\n","    print(\"\\n‚ö†Ô∏è  MANUAL RECOVERY NEEDED:\")\n","    print(\"   Run: spark.sql('ALTER TABLE Lake24.dbo.person_old RENAME TO Lake24.dbo.person')\")\n","    raise Exception(\"Table swap failed. Backup available.\")\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## STEP 1.4: POST-OPERATION VERIFICATION\n","\n","# COMMAND ----------\n","\n","def verify_completion():\n","    \"\"\"\n","    Comprehensive verification after schema change\n","    \n","    Verifies:\n","    1. Record count matches original\n","    2. All original columns present\n","    3. New audit columns present\n","    4. Sample data integrity\n","    5. Table is queryable\n","    \"\"\"\n","    print(\"\\n‚úÖ POST-OPERATION VERIFICATION:\")\n","    print(\"=\" * 80)\n","    \n","    try:\n","        # 1. Check table exists\n","        if not spark.catalog.tableExists(SOURCE_TABLE):\n","            print(f\"‚ùå ERROR: {SOURCE_TABLE} does not exist!\")\n","            return False\n","        print(f\"‚úÖ Table exists: {SOURCE_TABLE}\")\n","        \n","        # 2. Get new schema\n","        new_df = spark.table(SOURCE_TABLE)\n","        new_columns = new_df.columns\n","        \n","        print(f\"\\nüìä SCHEMA VERIFICATION:\")\n","        print(f\"   Original columns: {len(existing_columns)}\")\n","        print(f\"   New columns: {len(new_columns)}\")\n","        print(f\"   Added: {len(new_columns) - len(existing_columns)}\")\n","        \n","        # 3. Verify audit columns exist\n","        audit_columns = ['created_timestamp', 'updated_timestamp', 'is_deleted']\n","        for col in audit_columns:\n","            if col in new_columns:\n","                print(f\"   ‚úÖ {col} present\")\n","            else:\n","                print(f\"   ‚ùå {col} MISSING!\")\n","                return False\n","        \n","        # 4. Verify record count\n","        final_count = new_df.count()\n","        print(f\"\\nüìä RECORD COUNT VERIFICATION:\")\n","        print(f\"   Original: {record_count:,}\")\n","        print(f\"   Final: {final_count:,}\")\n","        \n","        if final_count != record_count:\n","            print(f\"   ‚ùå COUNT MISMATCH! Lost {record_count - final_count:,} records!\")\n","            return False\n","        print(f\"   ‚úÖ All {final_count:,} records preserved\")\n","        \n","        # 5. Sample data check\n","        print(f\"\\nüìä SAMPLE DATA CHECK:\")\n","        sample = new_df.select(\n","            \"person_id\", \n","            \"gender_concept_id\",\n","            \"created_timestamp\",\n","            \"updated_timestamp\", \n","            \"is_deleted\"\n","        ).limit(5)\n","        \n","        sample.show(truncate=False)\n","        \n","        # 6. Check audit column values\n","        null_created = new_df.filter(F.col(\"created_timestamp\").isNull()).count()\n","        null_updated = new_df.filter(F.col(\"updated_timestamp\").isNull()).count()\n","        deleted_count = new_df.filter(F.col(\"is_deleted\") == True).count()\n","        \n","        print(f\"\\nüìä AUDIT COLUMN STATISTICS:\")\n","        print(f\"   created_timestamp = NULL: {null_created:,} ({null_created/final_count*100:.1f}%)\")\n","        print(f\"   updated_timestamp = NULL: {null_updated:,} ({null_updated/final_count*100:.1f}%)\")\n","        print(f\"   is_deleted = True: {deleted_count:,} ({deleted_count/final_count*100:.1f}%)\")\n","        \n","        print(\"\\n\" + \"=\" * 80)\n","        print(\"‚úÖ‚úÖ‚úÖ PHASE 1 - STEP 1 COMPLETED SUCCESSFULLY ‚úÖ‚úÖ‚úÖ\")\n","        print(\"=\" * 80)\n","        print(f\"‚úÖ Table: {SOURCE_TABLE}\")\n","        print(f\"‚úÖ Records: {final_count:,} (preserved)\")\n","        print(f\"‚úÖ Columns: {len(new_columns)} (added {len(new_columns) - len(existing_columns)})\")\n","        print(f\"‚úÖ Backup: {BACKUP_TABLE} (available for 7 days)\")\n","        print(\"=\" * 80)\n","        \n","        return True\n","        \n","    except Exception as e:\n","        print(f\"‚ùå VERIFICATION FAILED: {str(e)}\")\n","        return False\n","\n","# Verify completion\n","verification_passed = verify_completion()\n","\n","if not verification_passed:\n","    print(\"\\n‚ö†Ô∏è  VERIFICATION FAILED BUT TABLE UPDATED\")\n","    print(\"   Table may be in inconsistent state\")\n","    print(\"   Backup available for recovery\")\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## CLEANUP & SUMMARY\n","\n","# COMMAND ----------\n","\n","print(\"\\nüßπ CLEANUP:\")\n","print(\"-\" * 80)\n","\n","# Cleanup temp table (if still exists)\n","try:\n","    if spark.catalog.tableExists(TEMP_TABLE):\n","        spark.sql(f\"DROP TABLE IF EXISTS {TEMP_TABLE}\")\n","        print(f\"‚úÖ Cleaned up: {TEMP_TABLE}\")\n","except:\n","    pass\n","\n","print(\"\\nüìã SUMMARY:\")\n","print(\"=\" * 80)\n","print(\"PHASE 1 - STEP 1: ADD AUDIT COLUMNS\")\n","print(\"=\" * 80)\n","print(f\"Status: {'‚úÖ SUCCESS' if verification_passed else '‚ùå FAILED'}\")\n","print(f\"Table: {SOURCE_TABLE}\")\n","print(f\"Records: {record_count:,} ‚Üí {spark.table(SOURCE_TABLE).count():,}\")\n","print(f\"Columns Added:\")\n","print(f\"  - created_timestamp (TIMESTAMP)\")\n","print(f\"  - updated_timestamp (TIMESTAMP)\")\n","print(f\"  - is_deleted (BOOLEAN)\")\n","print(f\"\\nBackup Location: {BACKUP_TABLE}\")\n","print(f\"Retention: Keep for 7 days\")\n","print(\"=\" * 80)\n","\n","print(\"\\nüìå NEXT STEPS:\")\n","print(\"1. ‚úÖ Audit columns added to person table\")\n","print(\"2. ‚è≠Ô∏è  Update synthetic data generator (Step 2)\")\n","print(\"3. ‚è≠Ô∏è  Create ETL control table (Step 3)\")\n","print(\"4. ‚è≠Ô∏è  Modify ETL for incremental load (Step 4)\")\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ---\n","# MAGIC ## ‚úÖ PHASE 1 - STEP 1 COMPLETE\n","# MAGIC \n","# MAGIC **What Was Done:**\n","# MAGIC - ‚úÖ Added 3 audit columns to person table\n","# MAGIC - ‚úÖ Preserved all 15.7M records\n","# MAGIC - ‚úÖ Created backup (safety)\n","# MAGIC - ‚úÖ Atomic table swap (no downtime)\n","# MAGIC - ‚úÖ Verified integrity\n","# MAGIC \n","# MAGIC **Rollback (if needed):**\n","# MAGIC ```python\n","# MAGIC # Restore from backup\n","# MAGIC spark.sql(f\"DROP TABLE IF EXISTS {SOURCE_TABLE}\")\n","# MAGIC spark.sql(f\"ALTER TABLE {BACKUP_TABLE} RENAME TO {SOURCE_TABLE}\")\n","# MAGIC ```\n","# MAGIC \n","# MAGIC **Impact on Your ETL v3.2:**\n","# MAGIC - Your SchemaInspector will detect new columns\n","# MAGIC - Will use ALTER TABLE ADD COLUMNS on downstream tables\n","# MAGIC - No FORCE_RECREATE needed (new columns compatible)\n","# MAGIC - Next ETL run will auto-evolve bronze/silver/gold/dim"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","spark_jobs_updating":false,"session_id":"23c0b5c7-6d7d-4afa-8adc-b5d549b56141","normalized_state":"finished","queued_time":"2026-02-28T23:07:09.5900099Z","session_start_time":null,"execution_start_time":"2026-02-28T23:07:09.5918662Z","execution_finish_time":"2026-02-28T23:08:26.5687784Z","parent_msg_id":"be40278e-c2be-4ad6-a3b5-817a67f4deda"},"text/plain":"StatementMeta(, 23c0b5c7-6d7d-4afa-8adc-b5d549b56141, 5, Finished, Available, Finished, False)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["================================================================================\nPHASE 1 - STEP 1: ADD AUDIT COLUMNS\n================================================================================\nSource Table: Lake24.dbo.person\nBackup Table: Lake24.dbo.person_backup_phase1\nSpark Version: 3.5.5.5.4.20260109.1\n================================================================================\n\nüîç PRE-FLIGHT CHECKS:\n--------------------------------------------------------------------------------\n‚úÖ Source table exists: Lake24.dbo.person\n‚úÖ Current record count: 15,712,818\n‚úÖ Current column count: 18\n‚úÖ No audit columns exist yet (will add: ['created_timestamp', 'updated_timestamp', 'is_deleted'])\nüìä Estimated processing time: 157.1 seconds (2.6 minutes)\n--------------------------------------------------------------------------------\n‚úÖ ALL PRE-FLIGHT CHECKS PASSED\n================================================================================\n\nüì¶ CREATING BACKUP:\n--------------------------------------------------------------------------------\n   Creating backup: Lake24.dbo.person_backup_phase1\n‚úÖ Backup created successfully\n   Records backed up: 15,712,818\n   Duration: 23.08 seconds\n   Location: Lake24.dbo.person_backup_phase1\n--------------------------------------------------------------------------------\n\nüîß ADDING AUDIT COLUMNS:\n--------------------------------------------------------------------------------\n   Step 1: Reading source table...\n   ‚úÖ Read 15,712,818 records\n   Step 2: Adding audit columns...\n   ‚úÖ Added: created_timestamp (TIMESTAMP, NULL for existing)\n   ‚úÖ Added: updated_timestamp (TIMESTAMP, NULL for existing)\n   ‚úÖ Added: is_deleted (BOOLEAN, False for existing)\n   ‚úÖ New schema has 21 columns (was 18)\n   Step 3: Writing to temporary table...\n   ‚úÖ Wrote 15,712,818 records to temp table\n   ‚úÖ Record count verified: 15,712,818 = 15,712,818\n‚úÖ Audit columns added successfully\n   Duration: 15.38 seconds\n--------------------------------------------------------------------------------\n\nüîÑ ATOMIC TABLE SWAP:\n--------------------------------------------------------------------------------\n   Step 1: Renaming Lake24.dbo.person ‚Üí person_old...\n   ‚úÖ Source renamed to person_old\n   Step 2: Renaming Lake24.dbo.person_temp_phase1 ‚Üí Lake24.dbo.person...\n   ‚úÖ Temp renamed to Lake24.dbo.person\n   ‚úÖ Verified: Lake24.dbo.person has 15,712,818 records\n   Step 3: Dropping person_old...\n   ‚úÖ Old table dropped\n--------------------------------------------------------------------------------\n‚úÖ TABLE SWAP COMPLETED SUCCESSFULLY\n================================================================================\n\n‚úÖ POST-OPERATION VERIFICATION:\n================================================================================\n‚úÖ Table exists: Lake24.dbo.person\n\nüìä SCHEMA VERIFICATION:\n   Original columns: 18\n   New columns: 21\n   Added: 3\n   ‚úÖ created_timestamp present\n   ‚úÖ updated_timestamp present\n   ‚úÖ is_deleted present\n\nüìä RECORD COUNT VERIFICATION:\n   Original: 15,712,818\n   Final: 15,712,818\n   ‚úÖ All 15,712,818 records preserved\n\nüìä SAMPLE DATA CHECK:\n+---------+-----------------+-----------------+-----------------+----------+\n|person_id|gender_concept_id|created_timestamp|updated_timestamp|is_deleted|\n+---------+-----------------+-----------------+-----------------+----------+\n|305726723|8532             |NULL             |NULL             |false     |\n|305726945|8532             |NULL             |NULL             |false     |\n|305729237|8532             |NULL             |NULL             |false     |\n|305731414|8532             |NULL             |NULL             |false     |\n|305732487|8532             |NULL             |NULL             |false     |\n+---------+-----------------+-----------------+-----------------+----------+\n\n\nüìä AUDIT COLUMN STATISTICS:\n   created_timestamp = NULL: 15,712,818 (100.0%)\n   updated_timestamp = NULL: 15,712,818 (100.0%)\n   is_deleted = True: 0 (0.0%)\n\n================================================================================\n‚úÖ‚úÖ‚úÖ PHASE 1 - STEP 1 COMPLETED SUCCESSFULLY ‚úÖ‚úÖ‚úÖ\n================================================================================\n‚úÖ Table: Lake24.dbo.person\n‚úÖ Records: 15,712,818 (preserved)\n‚úÖ Columns: 21 (added 3)\n‚úÖ Backup: Lake24.dbo.person_backup_phase1 (available for 7 days)\n================================================================================\n\nüßπ CLEANUP:\n--------------------------------------------------------------------------------\n\nüìã SUMMARY:\n================================================================================\nPHASE 1 - STEP 1: ADD AUDIT COLUMNS\n================================================================================\nStatus: ‚úÖ SUCCESS\nTable: Lake24.dbo.person\nRecords: 15,712,818 ‚Üí 15,712,818\nColumns Added:\n  - created_timestamp (TIMESTAMP)\n  - updated_timestamp (TIMESTAMP)\n  - is_deleted (BOOLEAN)\n\nBackup Location: Lake24.dbo.person_backup_phase1\nRetention: Keep for 7 days\n================================================================================\n\nüìå NEXT STEPS:\n1. ‚úÖ Audit columns added to person table\n2. ‚è≠Ô∏è  Update synthetic data generator (Step 2)\n3. ‚è≠Ô∏è  Create ETL control table (Step 3)\n4. ‚è≠Ô∏è  Modify ETL for incremental load (Step 4)\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"advisor":{"adviceMetadata":"{\"artifactId\":\"4ba5023b-2415-4f85-abf0-f1e9c2e7fd9a\",\"activityId\":\"23c0b5c7-6d7d-4afa-8adc-b5d549b56141\",\"applicationId\":\"application_1772317854751_0001\",\"jobGroupId\":\"5\",\"advices\":{\"warn\":2}}"}},"id":"e11a4362-ddff-458e-80e4-487d3c3b5693"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"5d277b6d-26ce-4249-a7d9-a6eac8face79"}],"default_lakehouse":"5d277b6d-26ce-4249-a7d9-a6eac8face79","default_lakehouse_name":"Lake24","default_lakehouse_workspace_id":"591f29e6-d45a-4989-9459-a5a7bf1b39b8"}}},"nbformat":4,"nbformat_minor":5}