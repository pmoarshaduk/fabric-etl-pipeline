{"cells":[{"cell_type":"code","source":["# Databricks notebook source\n","# MAGIC %md\n","# MAGIC # PHASE 1 - STEP 2: SYNTHETIC DATA GENERATOR (WITH AUDIT COLUMNS)\n","# MAGIC \n","# MAGIC **Version:** 2.0 (Phase 1 Compatible)\n","# MAGIC \n","# MAGIC **Changes from v1.0:**\n","# MAGIC - âœ… Added support for audit columns (created_timestamp, updated_timestamp, is_deleted)\n","# MAGIC - âœ… New records get proper timestamps (not NULL)\n","# MAGIC - âœ… Maintains backward compatibility\n","# MAGIC - âœ… Auto-detects schema changes\n","# MAGIC \n","# MAGIC **Purpose:** Generate synthetic person records for testing incremental load pattern\n","# MAGIC \n","# MAGIC **Compatible with:** Phase 1 ETL v4.0 (incremental load)\n","\n","# COMMAND ----------\n","\n","import pyspark.sql.functions as F\n","from pyspark.sql.types import *\n","import time\n","import builtins\n","from datetime import datetime\n","\n","print(\"=\" * 80)\n","print(\"SYNTHETIC DATA GENERATOR v2.0 - PHASE 1 COMPATIBLE\")\n","print(\"=\" * 80)\n","print(f\"Spark Version: {spark.version}\")\n","print(f\"Execution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","print(\"=\" * 80)\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## CONFIGURATION\n","\n","# COMMAND ----------\n","\n","# ==============================================================================\n","# CONFIGURATION\n","# ==============================================================================\n","ROWS_TO_GENERATE = 1000000  # Adjust as needed (tested up to 1M)\n","TARGET_TABLE = \"Lake24.dbo.person\"\n","\n","# Audit column behavior\n","ENABLE_AUDIT_COLUMNS = True  # Set to False to generate NULL (for testing)\n","\n","print(f\"ðŸ“‹ CONFIGURATION:\")\n","print(f\"   Target Table: {TARGET_TABLE}\")\n","print(f\"   Rows to Generate: {ROWS_TO_GENERATE:,}\")\n","print(f\"   Audit Columns: {'Enabled âœ…' if ENABLE_AUDIT_COLUMNS else 'Disabled (NULL)'}\")\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## STEP 1: LIVE METADATA ANALYSIS\n","\n","# COMMAND ----------\n","\n","print(f\"\\nðŸŽ¬ Starting generation of {ROWS_TO_GENERATE:,} records...\")\n","\n","# ==============================================================================\n","# LIVE METADATA ANALYSIS (DYNAMIC)\n","# ==============================================================================\n","print(f\"\\nðŸ” TABLE ANALYSIS:\")\n","print(\"-\" * 80)\n","\n","# Load the target table once to get both schema and current stats\n","target_table_df = spark.table(TARGET_TABLE)\n","actual_schema = target_table_df.schema\n","\n","# Get the LIVE count and MAX ID\n","stats_df = target_table_df.select(\n","    F.count(\"*\").alias(\"current_count\"), \n","    F.max(\"person_id\").alias(\"max_id\")\n",").collect()[0]\n","\n","initial_row_count = stats_df[\"current_count\"]\n","max_id_val = stats_df[\"max_id\"]\n","start_id = (max_id_val or 0) + 1\n","\n","print(f\"   Existing Records: {initial_row_count:,}\")\n","print(f\"   Current Max ID:   {max_id_val:,}\")\n","print(f\"   Next ID:          {start_id:,}\")\n","\n","# Check for audit columns\n","schema_columns = [f.name for f in actual_schema.fields]\n","audit_columns_present = {\n","    'created_timestamp': 'created_timestamp' in schema_columns,\n","    'updated_timestamp': 'updated_timestamp' in schema_columns,\n","    'is_deleted': 'is_deleted' in schema_columns\n","}\n","\n","print(f\"\\nðŸ“Š SCHEMA ANALYSIS:\")\n","print(f\"   Total Columns:      {len(schema_columns)}\")\n","print(f\"   created_timestamp:  {'âœ… Present' if audit_columns_present['created_timestamp'] else 'âŒ Missing'}\")\n","print(f\"   updated_timestamp:  {'âœ… Present' if audit_columns_present['updated_timestamp'] else 'âŒ Missing'}\")\n","print(f\"   is_deleted:         {'âœ… Present' if audit_columns_present['is_deleted'] else 'âŒ Missing'}\")\n","\n","if all(audit_columns_present.values()):\n","    print(f\"   Status: âœ… Phase 1 schema detected (audit columns present)\")\n","else:\n","    print(f\"   Status: âš ï¸  Pre-Phase 1 schema (audit columns missing)\")\n","    print(f\"   Note: Audit columns will be set to NULL\")\n","\n","print(\"-\" * 80)\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## STEP 2: VECTORIZED SYNTHETIC GENERATION\n","\n","# COMMAND ----------\n","\n","print(f\"\\nâš¡ GENERATING {ROWS_TO_GENERATE:,} SYNTHETIC RECORDS:\")\n","print(\"-\" * 80)\n","start_time = time.time()\n","\n","# ==============================================================================\n","# BASE SYNTHETIC GENERATION (Original 18 columns)\n","# ==============================================================================\n","\n","df_synth = spark.range(0, ROWS_TO_GENERATE) \\\n","    .withColumn(\"person_id\", (F.lit(start_id) + F.col(\"id\")).cast(IntegerType())) \\\n","    .withColumn(\"gender_rand\", F.rand()) \\\n","    .withColumn(\"gender_concept_id\", \n","        F.when(F.col(\"gender_rand\") > 0.52, 8532)\n","         .otherwise(8507)\n","         .cast(IntegerType())) \\\n","    .withColumn(\"year_of_birth\", (F.rand() * 70 + 1950).cast(IntegerType())) \\\n","    .withColumn(\"month_of_birth\", (F.rand() * 11 + 1).cast(IntegerType())) \\\n","    .withColumn(\"day_of_birth\", (F.rand() * 27 + 1).cast(IntegerType())) \\\n","    .withColumn(\"race_rand\", F.rand()) \\\n","    .withColumn(\"race_concept_id\", \n","        F.when(F.col(\"race_rand\") < 0.60, 8527)\n","         .when(F.col(\"race_rand\") < 0.74, 8515)\n","         .otherwise(0)\n","         .cast(IntegerType())) \\\n","    .withColumn(\"ethnicity_concept_id\", F.lit(38003564).cast(IntegerType())) \\\n","    .withColumn(\"person_source_value\", \n","        F.concat(F.lit(\"SYNTH_\"), F.col(\"person_id\").cast(\"string\"))) \\\n","    .withColumn(\"gender_source_value\", \n","        F.when(F.col(\"gender_concept_id\") == 8507, \"M\")\n","         .otherwise(\"F\")) \\\n","    .withColumn(\"gender_source_concept_id\", \n","        F.col(\"gender_concept_id\").cast(IntegerType())) \\\n","    .withColumn(\"birth_datetime\", \n","        F.to_date(F.concat_ws(\"-\", \n","            F.col(\"year_of_birth\"), \n","            F.col(\"month_of_birth\"), \n","            F.col(\"day_of_birth\"))))\n","\n","print(f\"   âœ… Generated base 18 columns\")\n","\n","# ==============================================================================\n","# ADD AUDIT COLUMNS (Phase 1 Enhancement)\n","# ==============================================================================\n","\n","if ENABLE_AUDIT_COLUMNS and all(audit_columns_present.values()):\n","    # Audit columns are present in schema AND enabled in config\n","    current_ts = datetime.now()\n","    \n","    df_synth = df_synth \\\n","        .withColumn(\"created_timestamp\", F.lit(current_ts).cast(TimestampType())) \\\n","        .withColumn(\"updated_timestamp\", F.lit(current_ts).cast(TimestampType())) \\\n","        .withColumn(\"is_deleted\", F.lit(False).cast(BooleanType()))\n","    \n","    print(f\"   âœ… Added audit columns with timestamps\")\n","    print(f\"      - created_timestamp: {current_ts}\")\n","    print(f\"      - updated_timestamp: {current_ts}\")\n","    print(f\"      - is_deleted: False\")\n","else:\n","    # Either disabled or columns don't exist - will be handled by schema alignment\n","    print(f\"   âš ï¸  Audit columns will be auto-filled (NULL or default)\")\n","\n","# ==============================================================================\n","# SCHEMA ALIGNMENT (Auto-adapts to target table schema)\n","# ==============================================================================\n","\n","print(f\"\\nðŸ”§ SCHEMA ALIGNMENT:\")\n","\n","final_columns = []\n","aligned_count = 0\n","auto_filled_count = 0\n","\n","for field in actual_schema.fields:\n","    if field.name in df_synth.columns:\n","        # Column exists in generated data - cast to target type\n","        final_columns.append(F.col(field.name).cast(field.dataType))\n","        aligned_count += 1\n","    else:\n","        # Column doesn't exist - auto-fill with NULL\n","        final_columns.append(F.lit(None).cast(field.dataType).alias(field.name))\n","        auto_filled_count += 1\n","        print(f\"   âš ï¸  Auto-filled: {field.name} â†’ NULL\")\n","\n","final_df = df_synth.select(final_columns)\n","\n","print(f\"   âœ… Aligned: {aligned_count} columns\")\n","if auto_filled_count > 0:\n","    print(f\"   âš ï¸  Auto-filled: {auto_filled_count} columns with NULL\")\n","else:\n","    print(f\"   âœ… Perfect match: All target columns populated\")\n","\n","generation_time = time.time() - start_time\n","print(f\"\\n   Generation Time: {generation_time:.2f} seconds\")\n","print(\"-\" * 80)\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## STEP 3: QUALITY CHECKS (Pre-Insert Validation)\n","\n","# COMMAND ----------\n","\n","print(f\"\\nðŸ” PRE-INSERT QUALITY CHECKS:\")\n","print(\"-\" * 80)\n","\n","try:\n","    # 1. Record count check\n","    generated_count = final_df.count()\n","    print(f\"   âœ… Record count: {generated_count:,} (expected: {ROWS_TO_GENERATE:,})\")\n","    \n","    if generated_count != ROWS_TO_GENERATE:\n","        print(f\"   âš ï¸  WARNING: Count mismatch!\")\n","    \n","    # 2. Sample data check\n","    print(f\"\\n   ðŸ“Š Sample data (first 3 records):\")\n","    sample_cols = [\"person_id\", \"gender_concept_id\", \"year_of_birth\"]\n","    if ENABLE_AUDIT_COLUMNS and all(audit_columns_present.values()):\n","        sample_cols.extend([\"created_timestamp\", \"updated_timestamp\", \"is_deleted\"])\n","    \n","    final_df.select(sample_cols).show(3, truncate=False)\n","    \n","    # 3. Null checks on key columns\n","    null_person_id = final_df.filter(F.col(\"person_id\").isNull()).count()\n","    null_gender = final_df.filter(F.col(\"gender_concept_id\").isNull()).count()\n","    \n","    print(f\"\\n   ðŸ“Š Null checks:\")\n","    print(f\"      person_id nulls: {null_person_id} {'âœ…' if null_person_id == 0 else 'âŒ'}\")\n","    print(f\"      gender_concept_id nulls: {null_gender} {'âœ…' if null_gender == 0 else 'âŒ'}\")\n","    \n","    # 4. Audit column validation\n","    if ENABLE_AUDIT_COLUMNS and all(audit_columns_present.values()):\n","        null_created = final_df.filter(F.col(\"created_timestamp\").isNull()).count()\n","        null_updated = final_df.filter(F.col(\"updated_timestamp\").isNull()).count()\n","        deleted_true = final_df.filter(F.col(\"is_deleted\") == True).count()\n","        \n","        print(f\"      created_timestamp nulls: {null_created} {'âœ…' if null_created == 0 else 'âš ï¸'}\")\n","        print(f\"      updated_timestamp nulls: {null_updated} {'âœ…' if null_updated == 0 else 'âš ï¸'}\")\n","        print(f\"      is_deleted = True: {deleted_true} {'âœ…' if deleted_true == 0 else 'âš ï¸'}\")\n","    \n","    print(f\"\\n   âœ… All quality checks passed\")\n","    \n","except Exception as e:\n","    print(f\"   âŒ Quality check failed: {str(e)}\")\n","    raise\n","\n","print(\"-\" * 80)\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## STEP 4: APPEND TO DELTA TABLE\n","\n","# COMMAND ----------\n","\n","print(f\"\\nðŸ’¾ WRITING TO TABLE:\")\n","print(\"-\" * 80)\n","\n","write_start = time.time()\n","\n","try:\n","    final_df.write \\\n","        .format(\"delta\") \\\n","        .mode(\"append\") \\\n","        .saveAsTable(TARGET_TABLE)\n","    \n","    write_time = time.time() - write_start\n","    print(f\"   âœ… SUCCESS: Write completed in {write_time:.2f} seconds\")\n","    \n","except Exception as e:\n","    print(f\"   âŒ ERROR: {str(e)}\")\n","    print(f\"   Rolling back...\")\n","    raise\n","\n","print(\"-\" * 80)\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## STEP 5: POST-INSERT VERIFICATION\n","\n","# COMMAND ----------\n","\n","print(f\"\\nðŸ” POST-INSERT VERIFICATION:\")\n","print(\"-\" * 80)\n","\n","# Get final count\n","final_count = spark.table(TARGET_TABLE).count()\n","expected_count = initial_row_count + ROWS_TO_GENERATE\n","\n","print(f\"   Initial rows:  {initial_row_count:,}\")\n","print(f\"   Added rows:    {ROWS_TO_GENERATE:,}\")\n","print(f\"   Expected:      {expected_count:,}\")\n","print(f\"   Actual:        {final_count:,}\")\n","\n","if final_count == expected_count:\n","    print(f\"   âœ… Count verification PASSED\")\n","else:\n","    print(f\"   âŒ Count verification FAILED\")\n","    print(f\"   Difference: {final_count - expected_count:,}\")\n","\n","# Verify last inserted records\n","print(f\"\\n   ðŸ“Š Verifying last inserted records:\")\n","last_records = spark.table(TARGET_TABLE) \\\n","    .filter(F.col(\"person_id\") >= start_id) \\\n","    .orderBy(F.col(\"person_id\").desc()) \\\n","    .limit(3)\n","\n","verify_cols = [\"person_id\", \"gender_concept_id\"]\n","if ENABLE_AUDIT_COLUMNS and all(audit_columns_present.values()):\n","    verify_cols.extend([\"created_timestamp\", \"is_deleted\"])\n","\n","last_records.select(verify_cols).show(3, truncate=False)\n","\n","print(\"-\" * 80)\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## FINAL SUMMARY\n","\n","# COMMAND ----------\n","\n","total_time = time.time() - start_time\n","throughput = ROWS_TO_GENERATE / total_time if total_time > 0 else 0\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"ðŸ“Š SYNTHETIC DATA GENERATION SUMMARY\")\n","print(\"=\" * 80)\n","print(f\"Target Table:        {TARGET_TABLE}\")\n","print(f\"Generator Version:   v2.0 (Phase 1 Compatible)\")\n","print(f\"\\nðŸ“ˆ COUNTS:\")\n","print(f\"   Initial Rows:     {initial_row_count:,}\")\n","print(f\"   Generated Rows:   {ROWS_TO_GENERATE:,}\")\n","print(f\"   Final Row Count:  {final_count:,}\")\n","print(f\"   Verification:     {'âœ… PASSED' if final_count == expected_count else 'âŒ FAILED'}\")\n","print(f\"\\nâš¡ PERFORMANCE:\")\n","print(f\"   Generation Time:  {generation_time:.2f}s\")\n","print(f\"   Write Time:       {write_time:.2f}s\")\n","print(f\"   Total Time:       {total_time:.2f}s\")\n","print(f\"   Throughput:       {throughput:,.0f} rows/sec\")\n","print(f\"\\nðŸ”§ AUDIT COLUMNS:\")\n","if ENABLE_AUDIT_COLUMNS and all(audit_columns_present.values()):\n","    print(f\"   Status:           âœ… Populated with timestamps\")\n","    print(f\"   created_timestamp: Current timestamp\")\n","    print(f\"   updated_timestamp: Current timestamp\")\n","    print(f\"   is_deleted:       False\")\n","else:\n","    print(f\"   Status:           âš ï¸  Set to NULL (pre-Phase 1 compatibility)\")\n","print(\"=\" * 80)\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## USAGE NOTES\n","# MAGIC \n","# MAGIC ### **Testing Incremental Load:**\n","# MAGIC \n","# MAGIC ```python\n","# MAGIC # Run 1: Generate 1M records\n","# MAGIC ROWS_TO_GENERATE = 1000000\n","# MAGIC ENABLE_AUDIT_COLUMNS = True\n","# MAGIC # All records get created_timestamp = NOW\n","# MAGIC \n","# MAGIC # Wait 5 minutes\n","# MAGIC \n","# MAGIC # Run 2: Generate another 100K records\n","# MAGIC ROWS_TO_GENERATE = 100000\n","# MAGIC ENABLE_AUDIT_COLUMNS = True\n","# MAGIC # New records get created_timestamp = NOW + 5 minutes\n","# MAGIC \n","# MAGIC # Now your ETL can detect:\n","# MAGIC # SELECT * FROM person WHERE created_timestamp > last_watermark\n","# MAGIC # Will return only the 100K new records âœ…\n","# MAGIC ```\n","# MAGIC \n","# MAGIC ### **Backward Compatibility:**\n","# MAGIC \n","# MAGIC ```python\n","# MAGIC # If running on pre-Phase 1 table (no audit columns):\n","# MAGIC ENABLE_AUDIT_COLUMNS = False\n","# MAGIC # Generator will work, audit columns set to NULL\n","# MAGIC ```\n","# MAGIC \n","# MAGIC ### **Performance Tuning:**\n","# MAGIC \n","# MAGIC ```python\n","# MAGIC # For < 1M rows:\n","# MAGIC ROWS_TO_GENERATE = 100000  # Fast (~1 second)\n","# MAGIC \n","# MAGIC # For 1-10M rows:\n","# MAGIC ROWS_TO_GENERATE = 5000000  # Medium (~15 seconds)\n","# MAGIC \n","# MAGIC # For 10M+ rows:\n","# MAGIC # Run multiple batches instead of one large batch\n","# MAGIC ```\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ---\n","# MAGIC ## âœ… PHASE 1 - STEP 2 COMPLETE\n","# MAGIC \n","# MAGIC **What Was Done:**\n","# MAGIC - âœ… Updated generator to support audit columns\n","# MAGIC - âœ… New records get proper timestamps (not NULL)\n","# MAGIC - âœ… Maintains backward compatibility\n","# MAGIC - âœ… Quality checks added\n","# MAGIC - âœ… Post-insert verification\n","# MAGIC \n","# MAGIC **What's Next:**\n","# MAGIC - â­ï¸ Step 3: Create ETL control/watermark table\n","# MAGIC - â­ï¸ Step 4: Implement incremental load in ETL v4.0\n","# MAGIC \n","# MAGIC **Testing Incremental Load:**\n","# MAGIC 1. Run this generator to create new records (they get timestamps)\n","# MAGIC 2. Run ETL v4.0 (Step 4) which will only process new records\n","# MAGIC 3. Verify 99% runtime reduction âœ…\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","spark_jobs_updating":false,"session_id":"efbdbbf8-4157-46bb-9b38-67c3a1256b91","normalized_state":"finished","queued_time":"2026-03-01T07:23:36.8349339Z","session_start_time":"2026-03-01T07:23:36.8364522Z","execution_start_time":"2026-03-01T07:23:49.8826258Z","execution_finish_time":"2026-03-01T07:24:17.4462954Z","parent_msg_id":"d7eef5ba-0488-47bd-ac2d-7208883966c3"},"text/plain":"StatementMeta(, efbdbbf8-4157-46bb-9b38-67c3a1256b91, 3, Finished, Available, Finished, False)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["================================================================================\nSYNTHETIC DATA GENERATOR v2.0 - PHASE 1 COMPATIBLE\n================================================================================\nSpark Version: 3.5.5.5.4.20260109.1\nExecution Time: 2026-03-01 07:23:49\n================================================================================\nðŸ“‹ CONFIGURATION:\n   Target Table: Lake24.dbo.person\n   Rows to Generate: 1,000,000\n   Audit Columns: Enabled âœ…\n\nðŸŽ¬ Starting generation of 1,000,000 records...\n\nðŸ” TABLE ANALYSIS:\n--------------------------------------------------------------------------------\n   Existing Records: 15,712,818\n   Current Max ID:   312,960,608\n   Next ID:          312,960,609\n\nðŸ“Š SCHEMA ANALYSIS:\n   Total Columns:      21\n   created_timestamp:  âœ… Present\n   updated_timestamp:  âœ… Present\n   is_deleted:         âœ… Present\n   Status: âœ… Phase 1 schema detected (audit columns present)\n--------------------------------------------------------------------------------\n\nâš¡ GENERATING 1,000,000 SYNTHETIC RECORDS:\n--------------------------------------------------------------------------------\n   âœ… Generated base 18 columns\n   âœ… Added audit columns with timestamps\n      - created_timestamp: 2026-03-01 07:24:04.931232\n      - updated_timestamp: 2026-03-01 07:24:04.931232\n      - is_deleted: False\n\nðŸ”§ SCHEMA ALIGNMENT:\n   âš ï¸  Auto-filled: location_id â†’ NULL\n   âš ï¸  Auto-filled: provider_id â†’ NULL\n   âš ï¸  Auto-filled: care_site_id â†’ NULL\n   âš ï¸  Auto-filled: race_source_value â†’ NULL\n   âš ï¸  Auto-filled: race_source_concept_id â†’ NULL\n   âš ï¸  Auto-filled: ethnicity_source_value â†’ NULL\n   âš ï¸  Auto-filled: ethnicity_source_concept_id â†’ NULL\n   âœ… Aligned: 14 columns\n   âš ï¸  Auto-filled: 7 columns with NULL\n\n   Generation Time: 0.62 seconds\n--------------------------------------------------------------------------------\n\nðŸ” PRE-INSERT QUALITY CHECKS:\n--------------------------------------------------------------------------------\n   âœ… Record count: 1,000,000 (expected: 1,000,000)\n\n   ðŸ“Š Sample data (first 3 records):\n+---------+-----------------+-------------+--------------------------+--------------------------+----------+\n|person_id|gender_concept_id|year_of_birth|created_timestamp         |updated_timestamp         |is_deleted|\n+---------+-----------------+-------------+--------------------------+--------------------------+----------+\n|312960609|8507             |1962         |2026-03-01 07:24:04.931232|2026-03-01 07:24:04.931232|false     |\n|312960610|8507             |2003         |2026-03-01 07:24:04.931232|2026-03-01 07:24:04.931232|false     |\n|312960611|8532             |1950         |2026-03-01 07:24:04.931232|2026-03-01 07:24:04.931232|false     |\n+---------+-----------------+-------------+--------------------------+--------------------------+----------+\nonly showing top 3 rows\n\n\n   ðŸ“Š Null checks:\n      person_id nulls: 0 âœ…\n      gender_concept_id nulls: 0 âœ…\n      created_timestamp nulls: 0 âœ…\n      updated_timestamp nulls: 0 âœ…\n      is_deleted = True: 0 âœ…\n\n   âœ… All quality checks passed\n--------------------------------------------------------------------------------\n\nðŸ’¾ WRITING TO TABLE:\n--------------------------------------------------------------------------------\n   âœ… SUCCESS: Write completed in 5.72 seconds\n--------------------------------------------------------------------------------\n\nðŸ” POST-INSERT VERIFICATION:\n--------------------------------------------------------------------------------\n   Initial rows:  15,712,818\n   Added rows:    1,000,000\n   Expected:      16,712,818\n   Actual:        16,712,818\n   âœ… Count verification PASSED\n\n   ðŸ“Š Verifying last inserted records:\n+---------+-----------------+--------------------------+----------+\n|person_id|gender_concept_id|created_timestamp         |is_deleted|\n+---------+-----------------+--------------------------+----------+\n|313960608|8507             |2026-03-01 07:24:04.931232|false     |\n|313960607|8532             |2026-03-01 07:24:04.931232|false     |\n|313960606|8532             |2026-03-01 07:24:04.931232|false     |\n+---------+-----------------+--------------------------+----------+\n\n--------------------------------------------------------------------------------\n\n================================================================================\nðŸ“Š SYNTHETIC DATA GENERATION SUMMARY\n================================================================================\nTarget Table:        Lake24.dbo.person\nGenerator Version:   v2.0 (Phase 1 Compatible)\n\nðŸ“ˆ COUNTS:\n   Initial Rows:     15,712,818\n   Generated Rows:   1,000,000\n   Final Row Count:  16,712,818\n   Verification:     âœ… PASSED\n\nâš¡ PERFORMANCE:\n   Generation Time:  0.62s\n   Write Time:       5.72s\n   Total Time:       12.29s\n   Throughput:       81,377 rows/sec\n\nðŸ”§ AUDIT COLUMNS:\n   Status:           âœ… Populated with timestamps\n   created_timestamp: Current timestamp\n   updated_timestamp: Current timestamp\n   is_deleted:       False\n================================================================================\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"83a2363d-769f-4038-a6fc-e2ef4d5e7609"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"5d277b6d-26ce-4249-a7d9-a6eac8face79"}],"default_lakehouse":"5d277b6d-26ce-4249-a7d9-a6eac8face79","default_lakehouse_name":"Lake24","default_lakehouse_workspace_id":"591f29e6-d45a-4989-9459-a5a7bf1b39b8"}}},"nbformat":4,"nbformat_minor":5}