{"cells":[{"cell_type":"code","source":["# Databricks notebook source\n","# MAGIC %md\n","# MAGIC # PRODUCTION ETL - v3.2\n","# MAGIC\n","# MAGIC **Changes from v3.1 (single change ‚Äî incremental):**\n","# MAGIC - FIX: audit_trail rows column type conflict (INT vs BIGINT) resolved correctly\n","# MAGIC - ROOT CAUSE: ALTER TABLE CHANGE COLUMN for type widening is unsupported in this\n","# MAGIC   Delta Lake protocol version. mergeSchema=true handles additive changes only ‚Äî\n","# MAGIC   it does not resolve type conflicts. Both approaches therefore failed.\n","# MAGIC - SOLUTION: Cast incoming rows value to IntegerType at write time to match the\n","# MAGIC   existing table schema. Eliminates the type conflict without DDL surgery.\n","# MAGIC   INT is safe for row counts at current 2.7M scale. When scale approaches 2.1B\n","# MAGIC   rows, schedule a planned FORCE_RECREATE during a maintenance window.\n","# MAGIC - REMOVED: ALTER TABLE CHANGE COLUMN attempt (unsupported, caused silent fallthrough)\n","# MAGIC - REMOVED: mergeSchema=true on audit_trail write (masks type conflicts, not a fix)\n","# MAGIC - Bronze table strategy note: MERGE on person_id is correct for a full extract pattern.\n","# MAGIC   Source deletions are not yet handled ‚Äî tracked as pending item for v4.0.\n","\n","# COMMAND ----------\n","\n","import pyspark.sql.functions as F\n","from pyspark.sql.types import *\n","from delta.tables import DeltaTable\n","from datetime import datetime\n","import hashlib\n","import json\n","import uuid\n","\n","print(\"=\" * 80)\n","print(\"PRODUCTION ETL - v3.0\")\n","print(\"=\" * 80)\n","print(f\"Spark:    {spark.version}\")\n","print(f\"Database: {spark.sql('SELECT current_database()').collect()[0][0]}\")\n","print(\"=\" * 80)\n","\n","# COMMAND ----------\n","# MAGIC %md\n","# MAGIC ## CONFIGURATION\n","\n","# COMMAND ----------\n","\n","class Config:\n","    \"\"\"Production configuration\"\"\"\n","    DATABASE         = \"dbo\"\n","    SOURCE_TABLE     = \"person\"\n","    PIPELINE_NAME    = \"person_etl_v3\"\n","    ENVIRONMENT      = \"PROD\"\n","\n","    # Performance tuning for 45M rows\n","    SHUFFLE_PARTITIONS = 400\n","    REPARTITION_COUNT  = 400\n","\n","    # Schema management\n","    # FORCE_RECREATE must be set explicitly by a human ‚Äî never toggled automatically.\n","    # When False (default): schema mismatches are handled via ALTER TABLE.\n","    # When True:            DROP + recreate is permitted for full reload scenarios.\n","    FORCE_RECREATE   = False\n","\n","    # Compliance\n","    DATA_CLASSIFICATION   = \"CONFIDENTIAL-PERSONAL\"\n","    NHS_VERSION           = \"v3.0\"\n","    NHS_UNKNOWN_GENDER    = 8551\n","    NHS_UNKNOWN_ETHNICITY = 7\n","    NHS_UNKNOWN_RACE      = 0\n","\n","    @staticmethod\n","    def table(name):\n","        return f\"{Config.DATABASE}.{name}\"\n","\n","\n","spark.conf.set(\"spark.sql.shuffle.partitions\", str(Config.SHUFFLE_PARTITIONS))\n","spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n","spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n","\n","print(f\"Config: Partitions={Config.SHUFFLE_PARTITIONS} | FORCE_RECREATE={Config.FORCE_RECREATE}\")\n","print(f\"Pipeline: {Config.PIPELINE_NAME} | Env: {Config.ENVIRONMENT}\")\n","\n","# COMMAND ----------\n","# MAGIC %md\n","# MAGIC ## UTILITIES\n","\n","# COMMAND ----------\n","\n","# Pseudonymization\n","def pseudonymize(value: str) -> str:\n","    if not value:\n","        return None\n","    return hashlib.sha256(f\"{value}FABRIC_2026\".encode()).hexdigest()\n","\n","pseudonymize_udf = F.udf(pseudonymize, StringType())\n","\n","\n","# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","# Schema Inspector ‚Äî v3.1\n","#\n","# PRINCIPLE: A schema change is a governance event, not an automatic fix.\n","#\n","# Behaviour:\n","#   New columns     ‚Üí ALTER TABLE ADD COLUMNS  (non-breaking, preserves all history)\n","#   Type conflicts  ‚Üí Safe cast to existing type + RCA warning\n","#                     If cast impossible ‚Üí FAILED (pipeline stops, human action required)\n","#   FORCE_RECREATE  ‚Üí DROP only when Config.FORCE_RECREATE = True (human decision)\n","#   No change       ‚Üí MERGE (normal incremental path)\n","#\n","# All schema events are written to audit_trail (event_type SCHEMA_*).\n","# Schema errors are written to rca_errors (category SCHEMA).\n","# Query schema history: SELECT * FROM dbo.audit_trail WHERE event_type LIKE 'SCHEMA%'\n","#\n","# What it will NEVER do automatically:\n","#   - DROP a table because of schema drift\n","#   - Silently swallow type conflicts\n","#   - Destroy Delta transaction log history\n","# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","class SchemaInspector:\n","\n","    @staticmethod\n","    def _type_str(dtype):\n","        return dtype.simpleString()\n","\n","    @staticmethod\n","    def validate_and_prepare(source_df, table_name, audit, rca, session_id):\n","        \"\"\"\n","        Returns (success: bool, prepared_df: DataFrame, action: str)\n","        action values: CREATE | MERGE | EVOLVED | RECREATE | FAILED\n","\n","        Schema events ‚Üí audit_trail  (event_type SCHEMA_*)\n","        Schema errors ‚Üí rca_errors   (category SCHEMA)\n","        \"\"\"\n","        try:\n","            # ‚îÄ‚îÄ Table does not exist ‚Üí CREATE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","            if not spark.catalog.tableExists(table_name):\n","                audit.log(\"SCHEMA_CHECK\",\n","                          f\"Table {table_name} does not exist ‚Äî will create\",\n","                          status=\"INFO\")\n","                return True, source_df, \"CREATE\"\n","\n","            existing_schema = {f.name: f.dataType for f in spark.table(table_name).schema}\n","            source_schema   = {f.name: f.dataType for f in source_df.schema}\n","\n","            new_columns    = {\n","                c: t for c, t in source_schema.items()\n","                if c not in existing_schema\n","            }\n","            type_conflicts = {\n","                c: (existing_schema[c], source_schema[c])\n","                for c in source_schema\n","                if c in existing_schema\n","                and SchemaInspector._type_str(existing_schema[c])\n","                != SchemaInspector._type_str(source_schema[c])\n","            }\n","\n","            evolved = False\n","\n","            # ‚îÄ‚îÄ Handle type conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","            if type_conflicts:\n","                conflict_detail = \"; \".join(\n","                    f\"{col}: table={SchemaInspector._type_str(old)} \"\n","                    f\"incoming={SchemaInspector._type_str(new)}\"\n","                    for col, (old, new) in type_conflicts.items()\n","                )\n","                audit.log(\"SCHEMA_TYPE_CONFLICT\",\n","                          f\"Type conflicts in {table_name}: {conflict_detail}\",\n","                          status=\"WARNING\")\n","                rca.capture_error(\n","                    \"SCHEMA\", \"TYPE_CONFLICT\", \"WARNING\", \"SCHEMA_VALIDATION\",\n","                    column=\", \".join(type_conflicts.keys()),\n","                    error_value=conflict_detail,\n","                    expected=\"Matching data types\",\n","                    rule=\"SCHEMA_TYPE_COMPATIBILITY\",\n","                    resolution=(\n","                        \"To allow full reload set Config.FORCE_RECREATE=True. \"\n","                        \"For column type changes raise a schema migration change request.\"\n","                    )\n","                )\n","\n","                if Config.FORCE_RECREATE:\n","                    audit.log(\"SCHEMA_RECREATE\",\n","                              f\"FORCE_RECREATE=True ‚Äî dropping {table_name}. \"\n","                              f\"WARNING: downstream views/semantic models may break.\",\n","                              status=\"WARNING\")\n","                    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n","                    return True, source_df, \"RECREATE\"\n","\n","                # Attempt safe cast ‚Äî cast incoming to match existing table types\n","                safe_df = source_df\n","                for col_name, (existing_type, _) in type_conflicts.items():\n","                    try:\n","                        safe_df = safe_df.withColumn(\n","                            col_name, F.col(col_name).cast(existing_type)\n","                        )\n","                        audit.log(\"SCHEMA_CAST\",\n","                                  f\"Cast {col_name} to {SchemaInspector._type_str(existing_type)} \"\n","                                  f\"to match existing table schema\",\n","                                  status=\"WARNING\")\n","                        evolved = True\n","                    except Exception as cast_err:\n","                        audit.log(\"SCHEMA_CAST_FAILED\",\n","                                  f\"Cannot cast {col_name}: {cast_err} ‚Äî pipeline stopped. \"\n","                                  f\"Raise a schema migration change request.\",\n","                                  status=\"FAILURE\")\n","                        return False, source_df, \"FAILED\"\n","                source_df = safe_df\n","\n","            # ‚îÄ‚îÄ Handle new columns ‚Äî ALTER TABLE (non-breaking) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","            if new_columns:\n","                for col_name, col_type in new_columns.items():\n","                    type_str = SchemaInspector._type_str(col_type)\n","                    try:\n","                        spark.sql(\n","                            f\"ALTER TABLE {table_name} \"\n","                            f\"ADD COLUMNS (`{col_name}` {type_str})\"\n","                        )\n","                        audit.log(\"SCHEMA_EVOLVED\",\n","                                  f\"Added column `{col_name}` ({type_str}) to {table_name} \"\n","                                  f\"‚Äî existing rows will have NULL for this column\",\n","                                  status=\"INFO\")\n","                        evolved = True\n","                    except Exception as alter_err:\n","                        audit.log(\"SCHEMA_ALTER_FAILED\",\n","                                  f\"ALTER TABLE failed for `{col_name}`: {alter_err}\",\n","                                  status=\"FAILURE\")\n","                        rca.capture_error(\n","                            \"SCHEMA\", \"ALTER_FAILED\", \"CRITICAL\", \"SCHEMA_VALIDATION\",\n","                            column=col_name,\n","                            error_value=str(alter_err),\n","                            resolution=\"Check table permissions and Delta Lake version\"\n","                        )\n","                        return False, source_df, \"FAILED\"\n","\n","            # ‚îÄ‚îÄ Return action ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","            if evolved:\n","                audit.log(\"SCHEMA_CHECK\",\n","                          f\"Schema evolved for {table_name} ‚Äî proceeding with MERGE\",\n","                          status=\"SUCCESS\")\n","                return True, source_df, \"EVOLVED\"\n","\n","            audit.log(\"SCHEMA_CHECK\",\n","                      f\"Schema compatible ‚Äî no changes for {table_name}\",\n","                      status=\"SUCCESS\")\n","            return True, source_df, \"MERGE\"\n","\n","        except Exception as e:\n","            audit.log(\"SCHEMA_ERROR\", f\"Schema validation error: {e}\", status=\"FAILURE\")\n","            return False, source_df, \"FAILED\"\n","\n","\n","# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","# RCA Engine\n","# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","class RCAEngine:\n","    def __init__(self, session_id):\n","        self.session_id = session_id\n","        self.errors     = []\n","\n","    def capture_error(self, category, error_type, severity, stage, **kwargs):\n","        self.errors.append({\n","            \"rca_id\":      str(uuid.uuid4()),\n","            \"timestamp\":   datetime.utcnow(),\n","            \"category\":    category,\n","            \"error_type\":  error_type,\n","            \"severity\":    severity,\n","            \"row_id\":      kwargs.get(\"row_id\"),\n","            \"column\":      kwargs.get(\"column\"),\n","            \"error_value\": str(kwargs.get(\"error_value\")) if kwargs.get(\"error_value\") else None,\n","            \"expected\":    kwargs.get(\"expected\"),\n","            \"rule\":        kwargs.get(\"rule\"),\n","            \"stage\":       stage,\n","            \"session_id\":  self.session_id,\n","            \"resolution\":  kwargs.get(\"resolution\", \"Review error\")\n","        })\n","\n","    def save(self):\n","        if not self.errors:\n","            return\n","        schema = StructType([\n","            StructField(\"rca_id\",      StringType(),    False),\n","            StructField(\"timestamp\",   TimestampType(), False),\n","            StructField(\"category\",    StringType(),    False),\n","            StructField(\"error_type\",  StringType(),    False),\n","            StructField(\"severity\",    StringType(),    False),\n","            StructField(\"row_id\",      StringType(),    True),\n","            StructField(\"column\",      StringType(),    True),\n","            StructField(\"error_value\", StringType(),    True),\n","            StructField(\"expected\",    StringType(),    True),\n","            StructField(\"rule\",        StringType(),    True),\n","            StructField(\"stage\",       StringType(),    False),\n","            StructField(\"session_id\",  StringType(),    False),\n","            StructField(\"resolution\",  StringType(),    True)\n","        ])\n","        data = [(\n","            e[\"rca_id\"], e[\"timestamp\"], e[\"category\"], e[\"error_type\"],\n","            e[\"severity\"], e[\"row_id\"], e[\"column\"], e[\"error_value\"],\n","            e[\"expected\"], e[\"rule\"], e[\"stage\"], e[\"session_id\"], e[\"resolution\"]\n","        ) for e in self.errors]\n","        df        = spark.createDataFrame(data, schema)\n","        rca_table = Config.table(\"rca_errors\")\n","        try:\n","            df.write.mode(\"append\").format(\"delta\").saveAsTable(rca_table)\n","            return\n","        except Exception as e1:\n","            print(f\"‚ö†Ô∏è RCA append failed: {e1}\")\n","        try:\n","            df.write.mode(\"append\").format(\"delta\") \\\n","              .option(\"mergeSchema\", \"true\").saveAsTable(rca_table)\n","            print(\"‚ö†Ô∏è RCA saved via mergeSchema ‚Äî check for schema drift\")\n","            return\n","        except Exception as e2:\n","            print(f\"‚ö†Ô∏è RCA mergeSchema failed: {e2}\")\n","        print(\"‚ùå WARNING: RCA falling back to overwrite ‚Äî historical records may be lost\")\n","        try:\n","            df.write.mode(\"overwrite\").format(\"delta\") \\\n","              .option(\"overwriteSchema\", \"true\").saveAsTable(rca_table)\n","        except Exception as e3:\n","            print(f\"‚ùå CRITICAL: RCA save failed entirely: {e3}\")\n","\n","\n","# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","# Audit Logger\n","# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","class AuditLogger:\n","    def __init__(self, session_id):\n","        self.session_id = session_id\n","        self.start_time = datetime.utcnow()\n","        self.events     = []\n","\n","    def log(self, event_type, description, stage=None, rows=0, status=\"SUCCESS\", **kwargs):\n","        duration = float(kwargs.get(\"duration\", 0.0))\n","        self.events.append({\n","            \"audit_id\":         str(uuid.uuid4()),\n","            \"session_id\":       self.session_id,\n","            \"timestamp\":        datetime.utcnow(),\n","            \"event_type\":       event_type,\n","            \"description\":      description,\n","            \"stage\":            stage,\n","            \"rows\":             int(rows),\n","            \"status\":           status,\n","            \"duration_seconds\": duration,\n","            \"metadata\":         json.dumps(kwargs.get(\"metadata\", {}))\n","        })\n","        icon = \"‚úÖ\" if status == \"SUCCESS\" else \"‚ö†Ô∏è\" if status == \"WARNING\" \\\n","               else \"‚ùå\" if status == \"FAILURE\" else \"‚ÑπÔ∏è\"\n","        print(f\"{icon} {event_type}: {description}\")\n","\n","    def save(self):\n","        \"\"\"\n","        Persists audit events to dbo.audit_trail.\n","\n","        Type compatibility strategy (v3.2):\n","        ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","        audit_trail was created in v1 with rows as IntegerType (INT).\n","        The incoming DataFrame now declares rows as LongType (BIGINT).\n","\n","        ALTER TABLE CHANGE COLUMN for type widening is unsupported in the\n","        Delta Lake protocol version running on this cluster. mergeSchema=true\n","        handles additive schema changes only ‚Äî it does not resolve type\n","        conflicts. Both approaches fail, as observed in production.\n","\n","        Correct resolution: detect the existing column type at runtime and\n","        align the outgoing DataFrame schema to match it before writing.\n","        This eliminates the conflict without DDL surgery or overwrite fallback.\n","\n","        When the table does not yet exist (first run), rows is written as\n","        IntegerType to establish a consistent baseline. INT is safe at the\n","        current 2.7M row scale. A planned FORCE_RECREATE during a maintenance\n","        window is the appropriate upgrade path when scale approaches 2.1B rows.\n","        \"\"\"\n","        if not self.events:\n","            return\n","\n","        audit_table = Config.table(\"audit_trail\")\n","\n","        # Determine the rows column type in the existing table, if present.\n","        # Default to IntegerType to match the baseline established in v1.\n","        if spark.catalog.tableExists(audit_table):\n","            existing_rows_fields = [\n","                f.dataType for f in spark.table(audit_table).schema\n","                if f.name == \"rows\"\n","            ]\n","            rows_type = existing_rows_fields[0] if existing_rows_fields else IntegerType()\n","        else:\n","            rows_type = IntegerType()\n","\n","        schema = StructType([\n","            StructField(\"audit_id\",         StringType(),    False),\n","            StructField(\"session_id\",       StringType(),    False),\n","            StructField(\"timestamp\",        TimestampType(), False),\n","            StructField(\"event_type\",       StringType(),    False),\n","            StructField(\"description\",      StringType(),    False),\n","            StructField(\"stage\",            StringType(),    True),\n","            StructField(\"rows\",             rows_type,       True),  # matched to existing table\n","            StructField(\"status\",           StringType(),    False),\n","            StructField(\"duration_seconds\", DoubleType(),    True),\n","            StructField(\"metadata\",         StringType(),    True)\n","        ])\n","\n","        # Cast rows values to match the target type (int or long) before DataFrame creation.\n","        # Python int is compatible with both IntegerType and LongType in PySpark.\n","        data = [(\n","            e[\"audit_id\"], e[\"session_id\"], e[\"timestamp\"], e[\"event_type\"],\n","            e[\"description\"], e[\"stage\"], int(e[\"rows\"]), e[\"status\"],\n","            e[\"duration_seconds\"], e[\"metadata\"]\n","        ) for e in self.events]\n","\n","        df = spark.createDataFrame(data, schema)\n","\n","        # Standard append ‚Äî no mergeSchema required because the schema is now\n","        # guaranteed to match the existing table exactly.\n","        try:\n","            df.write.mode(\"append\").format(\"delta\").saveAsTable(audit_table)\n","        except Exception as e1:\n","            # Second attempt: mergeSchema handles the case where other columns\n","            # have drifted (e.g. new columns added by a future pipeline version).\n","            print(f\"‚ö†Ô∏è AUDIT_TRAIL: Standard append failed ({e1}) ‚Äî retrying with mergeSchema\")\n","            try:\n","                df.write.mode(\"append\").format(\"delta\") \\\n","                  .option(\"mergeSchema\", \"true\").saveAsTable(audit_table)\n","            except Exception as e2:\n","                # Last resort ‚Äî explicit, never silent.\n","                # This path should not be reachable under normal operations.\n","                print(f\"‚ùå AUDIT_TRAIL: mergeSchema append failed ({e2})\")\n","                print(\"‚ùå AUDIT_TRAIL: WARNING ‚Äî falling back to overwrite. \"\n","                      \"Historical audit records will be lost. \"\n","                      \"Investigate root cause before next pipeline execution.\")\n","                df.write.mode(\"overwrite\").format(\"delta\") \\\n","                  .option(\"overwriteSchema\", \"true\").saveAsTable(audit_table)\n","\n","    def get_summary(self):\n","        duration = (datetime.utcnow() - self.start_time).total_seconds()\n","        return {\n","            \"session_id\": self.session_id,\n","            \"duration\":   duration,\n","            \"events\":     len(self.events),\n","            \"success\":    sum(1 for e in self.events if e[\"status\"] == \"SUCCESS\"),\n","            \"failure\":    sum(1 for e in self.events if e[\"status\"] == \"FAILURE\")\n","        }\n","\n","\n","# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","# Data Quality\n","# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","def apply_dq_checks(df, rules, audit):\n","    df_dq = df.withColumn(\"dq_status\",   F.lit(\"VALID\")) \\\n","              .withColumn(\"dq_failures\", F.array().cast(ArrayType(StringType())))\n","\n","    for rule in rules:\n","        df_dq = df_dq \\\n","            .withColumn(\"dq_status\",\n","                F.when(~rule[\"condition\"], F.lit(\"ERROR\"))\n","                 .otherwise(F.col(\"dq_status\"))) \\\n","            .withColumn(\"dq_failures\",\n","                F.when(~rule[\"condition\"],\n","                       F.array_union(F.col(\"dq_failures\"), F.array(F.lit(rule[\"name\"]))))\n","                .otherwise(F.col(\"dq_failures\")))\n","\n","    valid_df      = df_dq.filter(F.col(\"dq_status\") == \"VALID\")\n","    quarantine_df = df_dq.filter(F.col(\"dq_status\") != \"VALID\")\n","    total         = df.count()\n","    valid         = valid_df.count()\n","    pass_rate     = round((valid / total) * 100, 2) if total > 0 else 0\n","\n","    audit.log(\"DQ_VALIDATION\", f\"Pass rate: {pass_rate}%\", \"SILVER\", total)\n","    return valid_df, quarantine_df, {\"total\": total, \"valid\": valid, \"pass_rate\": pass_rate}\n","\n","\n","# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","# NHS Rules\n","# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n","def apply_nhs_rules(df):\n","    df = df.withColumn(\"gender_concept_id_clean\",\n","            F.when(F.col(\"gender_concept_id\").isNull(), F.lit(Config.NHS_UNKNOWN_GENDER))\n","             .when(~F.col(\"gender_concept_id\").isin([8507, 8532]), F.lit(Config.NHS_UNKNOWN_GENDER))\n","             .otherwise(F.col(\"gender_concept_id\")))\n","\n","    df = df.withColumn(\"race_concept_id_clean\",\n","            F.when(F.col(\"race_concept_id\").isNull(), F.lit(Config.NHS_UNKNOWN_RACE))\n","             .otherwise(F.col(\"race_concept_id\")))\n","\n","    df = df.withColumn(\"ethnicity_concept_id_clean\",\n","            F.when(F.col(\"ethnicity_concept_id\").isNull(), F.lit(Config.NHS_UNKNOWN_ETHNICITY))\n","             .when(F.col(\"ethnicity_concept_id\") == 0, F.lit(Config.NHS_UNKNOWN_ETHNICITY))\n","             .otherwise(F.col(\"ethnicity_concept_id\")))\n","\n","    df = df.withColumn(\"birth_date\",\n","            F.when(\n","                F.col(\"year_of_birth\").isNotNull() &\n","                F.col(\"month_of_birth\").isNotNull() &\n","                F.col(\"day_of_birth\").isNotNull(),\n","                F.make_date(F.col(\"year_of_birth\"), F.col(\"month_of_birth\"), F.col(\"day_of_birth\"))\n","            ).otherwise(None))\n","\n","    df = df.withColumn(\"age_years\",\n","            F.floor(F.months_between(F.current_date(), F.col(\"birth_date\")) / 12))\n","\n","    df = df.withColumn(\"nhs_age_band\",\n","            F.when(F.col(\"age_years\") < 1,               \"0-<1\")\n","             .when(F.col(\"age_years\").between(1,  4),     \"1-4\")\n","             .when(F.col(\"age_years\").between(5,  9),     \"5-9\")\n","             .when(F.col(\"age_years\").between(10, 14),    \"10-14\")\n","             .when(F.col(\"age_years\").between(15, 19),    \"15-19\")\n","             .when(F.col(\"age_years\").between(20, 24),    \"20-24\")\n","             .when(F.col(\"age_years\").between(25, 29),    \"25-29\")\n","             .when(F.col(\"age_years\").between(30, 34),    \"30-34\")\n","             .when(F.col(\"age_years\").between(35, 39),    \"35-39\")\n","             .when(F.col(\"age_years\").between(40, 44),    \"40-44\")\n","             .when(F.col(\"age_years\").between(45, 49),    \"45-49\")\n","             .when(F.col(\"age_years\").between(50, 54),    \"50-54\")\n","             .when(F.col(\"age_years\").between(55, 59),    \"55-59\")\n","             .when(F.col(\"age_years\").between(60, 64),    \"60-64\")\n","             .when(F.col(\"age_years\").between(65, 69),    \"65-69\")\n","             .when(F.col(\"age_years\").between(70, 74),    \"70-74\")\n","             .when(F.col(\"age_years\").between(75, 79),    \"75-79\")\n","             .when(F.col(\"age_years\").between(80, 84),    \"80-84\")\n","             .when(F.col(\"age_years\") >= 85,              \"85+\")\n","             .otherwise(\"Unknown\"))\n","\n","    df = df.withColumn(\"ecds_compliant\", F.lit(True)) \\\n","           .withColumn(\"ecds_version\",   F.lit(Config.NHS_VERSION))\n","    return df\n","\n","\n","print(\"‚úÖ Utilities loaded\")\n","\n","# COMMAND ----------\n","# MAGIC %md\n","# MAGIC ## MAIN ETL PIPELINE ‚Äî v3.0\n","\n","# COMMAND ----------\n","\n","def run_production_etl():\n","\n","    session_id = str(uuid.uuid4())\n","\n","    print(\"\\n\" + \"=\" * 80)\n","    print(\"PRODUCTION ETL v3.2 ‚Äî EXECUTION\")\n","    print(\"=\" * 80)\n","    print(f\"Session:       {session_id}\")\n","    print(f\"Pipeline:      {Config.PIPELINE_NAME}\")\n","    print(f\"Environment:   {Config.ENVIRONMENT}\")\n","    print(f\"ForceRecreate: {Config.FORCE_RECREATE}\")\n","    print(\"=\" * 80)\n","\n","    audit     = AuditLogger(session_id)\n","    rca       = RCAEngine(session_id)\n","    inspector = SchemaInspector()\n","\n","    audit.log(\"PIPELINE_START\", f\"ETL v3.2 started | {Config.PIPELINE_NAME}\", \"INIT\")\n","\n","    table_actions = {}\n","\n","    try:\n","        # =================================================================\n","        # BRONZE ‚Äî Raw Ingestion\n","        # =================================================================\n","        print(\"\\n[BRONZE] Raw ingestion...\")\n","        start_time = datetime.utcnow()\n","\n","        source_df = spark.table(Config.table(Config.SOURCE_TABLE))\n","\n","        bronze_df = source_df \\\n","            .withColumn(\"ingestion_timestamp\",  F.current_timestamp()) \\\n","            .withColumn(\"pipeline_run_id\",      F.lit(session_id)) \\\n","            .filter(F.col(\"person_id\").isNotNull()) \\\n","            .withColumn(\"lineage_source\",       F.lit(f\"dbo.{Config.SOURCE_TABLE}\")) \\\n","            .withColumn(\"lineage_pipeline\",     F.lit(Config.PIPELINE_NAME)) \\\n","            .withColumn(\"lineage_environment\",  F.lit(Config.ENVIRONMENT)) \\\n","            .withColumn(\"lineage_bronze_ts\",    F.current_timestamp())\n","\n","        bronze_df    = bronze_df.repartition(Config.REPARTITION_COUNT)\n","        bronze_count = bronze_df.count()\n","        audit.log(\"BRONZE_LOADED\",\n","                  f\"Loaded {bronze_count:,} records from {Config.SOURCE_TABLE}\",\n","                  \"BRONZE\", bronze_count)\n","\n","        bronze_table = Config.table(\"bronze_person\")\n","        success, prepared_df, action = inspector.validate_and_prepare(\n","            bronze_df, bronze_table, audit, rca, session_id\n","        )\n","        table_actions[\"bronze_person\"] = action\n","\n","        if not success:\n","            raise ValueError(\"Bronze schema validation failed ‚Äî check rca_errors table\")\n","\n","        if action in [\"CREATE\", \"RECREATE\"]:\n","            prepared_df.write \\\n","                .format(\"delta\") \\\n","                .mode(\"overwrite\") \\\n","                .option(\"overwriteSchema\", \"true\") \\\n","                .saveAsTable(bronze_table)\n","            audit.log(\"BRONZE_WRITE\", f\"{action}: {bronze_table}\", \"BRONZE\", bronze_count)\n","        else:\n","            # FIX 1: MERGE key is person_id only ‚Äî no pipeline_run_id\n","            target = DeltaTable.forName(spark, bronze_table)\n","            target.alias(\"target\").merge(\n","                prepared_df.alias(\"source\"),\n","                \"target.person_id = source.person_id\"\n","            ).whenMatchedUpdateAll() \\\n","             .whenNotMatchedInsertAll() \\\n","             .execute()\n","            audit.log(\"BRONZE_MERGE\", f\"Merged into {bronze_table}\", \"BRONZE\", bronze_count)\n","\n","        spark.sql(f\"OPTIMIZE {bronze_table}\")\n","\n","        end_time   = datetime.utcnow()\n","        duration   = (end_time - start_time).total_seconds()\n","        throughput = bronze_count / duration if duration > 0 else 0\n","        audit.log(\"BRONZE_COMPLETE\",\n","                  f\"Bronze complete: {bronze_count:,} records in {duration:.2f}s ({throughput:.0f} rows/s)\",\n","                  \"BRONZE\", bronze_count, duration=duration)\n","\n","        # =================================================================\n","        # SILVER ‚Äî Validation & Enrichment\n","        # =================================================================\n","        print(\"\\n[SILVER] Validation & enrichment...\")\n","        start_time = datetime.utcnow()\n","\n","        dq_rules = [\n","            {\"name\": \"PERSON_ID_NOT_NULL\", \"condition\": F.col(\"person_id\").isNotNull()},\n","            {\"name\": \"GENDER_VALID\", \"condition\":\n","                F.col(\"gender_concept_id\").isin([8507, 8532, 8551]) |\n","                F.col(\"gender_concept_id\").isNull()},\n","            {\"name\": \"BIRTH_YEAR_RANGE\", \"condition\":\n","                F.col(\"year_of_birth\").between(1900, 2026) |\n","                F.col(\"year_of_birth\").isNull()}\n","        ]\n","\n","        silver_valid_df, quarantine_df, dq_metrics = apply_dq_checks(\n","            bronze_df, dq_rules, audit\n","        )\n","\n","        print(f\"   DQ Pass Rate: {dq_metrics['pass_rate']}%\")\n","        print(f\"   Valid: {dq_metrics['valid']:,} | Quarantine: {dq_metrics['total'] - dq_metrics['valid']:,}\")\n","\n","        silver_df = apply_nhs_rules(silver_valid_df)\n","\n","        if \"person_source_value\" in silver_df.columns:\n","            silver_df = silver_df.withColumn(\n","                \"person_source_value_pseudo\",\n","                pseudonymize_udf(F.col(\"person_source_value\"))\n","            )\n","            audit.log(\"PSEUDONYMIZATION\", \"Applied GDPR pseudonymization\", \"SILVER\")\n","\n","        silver_df = silver_df \\\n","            .withColumn(\"silver_timestamp\",  F.current_timestamp()) \\\n","            .withColumn(\"lineage_silver_ts\", F.current_timestamp())\n","\n","        silver_df    = silver_df.repartition(Config.REPARTITION_COUNT)\n","        silver_table = Config.table(\"silver_person\")\n","        success, prepared_df, action = inspector.validate_and_prepare(\n","            silver_df, silver_table, audit, rca, session_id\n","        )\n","        table_actions[\"silver_person\"] = action\n","\n","        if not success:\n","            raise ValueError(\"Silver schema validation failed ‚Äî check rca_errors table\")\n","\n","        if action in [\"CREATE\", \"RECREATE\"]:\n","            prepared_df.write \\\n","                .format(\"delta\") \\\n","                .mode(\"overwrite\") \\\n","                .option(\"overwriteSchema\", \"true\") \\\n","                .saveAsTable(silver_table)\n","        else:\n","            target = DeltaTable.forName(spark, silver_table)\n","            target.alias(\"target\").merge(\n","                prepared_df.alias(\"source\"),\n","                \"target.person_id = source.person_id\"\n","            ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n","\n","        spark.sql(f\"OPTIMIZE {silver_table}\")\n","\n","        end_time = datetime.utcnow()\n","        audit.log(\"SILVER_COMPLETE\",\n","                  f\"Silver complete: {dq_metrics['valid']:,} records\",\n","                  \"SILVER\", dq_metrics['valid'],\n","                  duration=(end_time - start_time).total_seconds())\n","\n","        if dq_metrics['total'] - dq_metrics['valid'] > 0:\n","            quarantine_table = Config.table(\n","                f\"quarantine_person_{datetime.now().strftime('%Y%m%d')}\"\n","            )\n","            quarantine_df.write.mode(\"append\").format(\"delta\").saveAsTable(quarantine_table)\n","            audit.log(\"QUARANTINE_SAVED\",\n","                      f\"Quarantined {dq_metrics['total'] - dq_metrics['valid']:,} records\",\n","                      \"SILVER\")\n","\n","        # =================================================================\n","        # GOLD ‚Äî Business Layer\n","        # =================================================================\n","        print(\"\\n[GOLD] Business layer...\")\n","        start_time = datetime.utcnow()\n","\n","        if \"person_source_value_pseudo\" in silver_df.columns:\n","            person_key_col = F.col(\"person_source_value_pseudo\").alias(\"person_key\")\n","            person_key_src = \"person_source_value_pseudo\"\n","        else:\n","            person_key_col = F.col(\"person_id\").cast(StringType()).alias(\"person_key\")\n","            person_key_src = \"person_id (fallback ‚Äî person_source_value not present)\"\n","\n","        audit.log(\"GOLD_KEY\", f\"person_key sourced from: {person_key_src}\", \"GOLD\")\n","\n","        gold_df = silver_df.select(\n","            F.col(\"person_id\"),\n","            person_key_col,\n","            F.col(\"gender_concept_id_clean\").alias(\"gender_concept_id\"),\n","            F.col(\"age_years\"),\n","            F.col(\"nhs_age_band\"),\n","            F.col(\"ecds_compliant\"),\n","            F.col(\"lineage_source\"),\n","            F.col(\"lineage_pipeline\"),\n","            F.col(\"lineage_environment\"),\n","            F.col(\"lineage_bronze_ts\"),\n","            F.col(\"lineage_silver_ts\"),\n","            F.current_timestamp().alias(\"lineage_gold_ts\"),\n","            F.lit(session_id).alias(\"lineage_session_id\"),\n","            F.current_timestamp().alias(\"gold_created\")\n","        )\n","\n","        gold_df    = gold_df.repartition(Config.REPARTITION_COUNT)\n","        gold_table = Config.table(\"gold_person\")\n","        success, prepared_df, action = inspector.validate_and_prepare(\n","            gold_df, gold_table, audit, rca, session_id\n","        )\n","        table_actions[\"gold_person\"] = action\n","\n","        if not success:\n","            raise ValueError(\"Gold schema validation failed ‚Äî check rca_errors table\")\n","\n","        if action in [\"CREATE\", \"RECREATE\"]:\n","            prepared_df.write.format(\"delta\").mode(\"overwrite\") \\\n","                .option(\"overwriteSchema\", \"true\").saveAsTable(gold_table)\n","        else:\n","            target = DeltaTable.forName(spark, gold_table)\n","            target.alias(\"target\").merge(\n","                prepared_df.alias(\"source\"),\n","                \"target.person_id = source.person_id\"\n","            ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n","\n","        spark.sql(f\"OPTIMIZE {gold_table}\")\n","        gold_count = spark.table(gold_table).count()\n","\n","        end_time = datetime.utcnow()\n","        audit.log(\"GOLD_COMPLETE\", f\"Gold complete: {gold_count:,} records\",\n","                  \"GOLD\", gold_count,\n","                  duration=(end_time - start_time).total_seconds())\n","\n","        # =================================================================\n","        # DIMENSION ‚Äî SCD Type 2 (proper two-step expire + insert)\n","        # =================================================================\n","        print(\"\\n[DIM] Dimension (SCD Type 2)...\")\n","        start_time = datetime.utcnow()\n","\n","        dim_new_df = gold_df.select(\n","            F.col(\"person_id\"),\n","            F.col(\"person_key\"),\n","            F.col(\"gender_concept_id\"),\n","            F.col(\"age_years\"),\n","            F.col(\"nhs_age_band\"),\n","            F.col(\"ecds_compliant\"),\n","            F.col(\"lineage_session_id\")\n","        ).withColumn(\"effective_from\", F.current_date()) \\\n","         .withColumn(\"effective_to\",   F.lit(\"9999-12-31\").cast(\"date\")) \\\n","         .withColumn(\"is_current\",     F.lit(True))\n","\n","        dim_table = Config.table(\"dim_person\")\n","        success, prepared_df, action = inspector.validate_and_prepare(\n","            dim_new_df, dim_table, audit, rca, session_id\n","        )\n","        table_actions[\"dim_person\"] = action\n","\n","        if not success:\n","            raise ValueError(\"Dimension schema validation failed ‚Äî check rca_errors table\")\n","\n","        if action in [\"CREATE\", \"RECREATE\"]:\n","            prepared_df.write.format(\"delta\").mode(\"overwrite\") \\\n","                .option(\"overwriteSchema\", \"true\").saveAsTable(dim_table)\n","            audit.log(\"DIM_CREATE\", f\"{action}: {dim_table}\", \"DIM\")\n","        else:\n","            # FIX 2: Proper SCD2 ‚Äî two-step expire then insert\n","            target = DeltaTable.forName(spark, dim_table)\n","\n","            # Step 1: Expire changed current records\n","            target.alias(\"target\").merge(\n","                prepared_df.alias(\"source\"),\n","                \"target.person_id = source.person_id AND target.is_current = true\"\n","            ).whenMatchedUpdate(\n","                condition=\"\"\"\n","                    target.gender_concept_id != source.gender_concept_id OR\n","                    target.nhs_age_band       != source.nhs_age_band      OR\n","                    target.ecds_compliant     != source.ecds_compliant\n","                \"\"\",\n","                set={\n","                    \"is_current\":   F.lit(False),\n","                    \"effective_to\": F.current_date()\n","                }\n","            ).execute()\n","            audit.log(\"DIM_SCD2_EXPIRE\",\n","                      \"Step 1: expired changed current records\", \"DIM\")\n","\n","            # Step 2: Insert new current versions for changed + new persons\n","            target = DeltaTable.forName(spark, dim_table)\n","            target.alias(\"target\").merge(\n","                prepared_df.alias(\"source\"),\n","                \"target.person_id = source.person_id AND target.is_current = true\"\n","            ).whenNotMatchedInsertAll().execute()\n","            audit.log(\"DIM_SCD2_INSERT\",\n","                      \"Step 2: inserted new current versions\", \"DIM\")\n","\n","        spark.sql(f\"OPTIMIZE {dim_table}\")\n","        dim_count         = spark.table(dim_table).count()\n","        dim_current_count = spark.table(dim_table).filter(F.col(\"is_current\") == True).count()\n","        dim_expired_count = dim_count - dim_current_count\n","\n","        end_time = datetime.utcnow()\n","        audit.log(\"DIM_COMPLETE\",\n","                  f\"Dimension complete: {dim_count:,} total | \"\n","                  f\"{dim_current_count:,} current | {dim_expired_count:,} expired\",\n","                  \"DIM\", dim_count,\n","                  duration=(end_time - start_time).total_seconds())\n","\n","        # =================================================================\n","        # SUMMARY\n","        # =================================================================\n","        total_duration = (datetime.utcnow() - audit.start_time).total_seconds()\n","\n","        print(\"\\n\" + \"=\" * 80)\n","        print(\"‚úÖ‚úÖ‚úÖ PIPELINE SUCCESS ‚úÖ‚úÖ‚úÖ\")\n","        print(\"=\" * 80)\n","        print(f\"Session:     {session_id}\")\n","        print(f\"Duration:    {total_duration:.2f}s\")\n","        print(f\"Bronze:      {bronze_count:,}\")\n","        print(f\"Silver:      {dq_metrics['valid']:,} (DQ: {dq_metrics['pass_rate']}%)\")\n","        print(f\"Gold:        {gold_count:,}\")\n","        print(f\"Dimension:   {dim_count:,} total | {dim_current_count:,} current | {dim_expired_count:,} expired\")\n","        print(f\"Throughput:  {int(bronze_count / total_duration):,} rows/s\")\n","        print(f\"NHS ECDS:    {Config.NHS_VERSION} ‚úÖ | GDPR: Pseudonymized ‚úÖ\")\n","        print(f\"Actions:     {table_actions}\")\n","        print(\"=\" * 80)\n","\n","        audit.log(\"PIPELINE_COMPLETE\", \"Pipeline completed successfully\", \"COMPLETE\",\n","                  metadata={\n","                      \"bronze\": bronze_count, \"silver\": dq_metrics['valid'],\n","                      \"gold\": gold_count, \"dimension\": dim_count,\n","                      \"dim_current\": dim_current_count, \"dim_expired\": dim_expired_count,\n","                      \"duration\": total_duration, \"table_actions\": table_actions\n","                  })\n","\n","    except Exception as e:\n","        audit.log(\"PIPELINE_FAILURE\", f\"Pipeline failed: {str(e)}\", status=\"FAILURE\")\n","        rca.capture_error(\"SYSTEM\", type(e).__name__, \"CRITICAL\", \"PIPELINE\",\n","                          error_value=str(e), resolution=\"Review logs and rca_errors table\")\n","        print(f\"\\n‚ùå Pipeline failed: {str(e)}\")\n","\n","    finally:\n","        audit.save()\n","        rca.save()\n","        summary = audit.get_summary()\n","        print(f\"\\nüìä Session Summary:\")\n","        print(f\"   Duration: {summary['duration']:.2f}s\")\n","        print(f\"   Events:   {summary['events']}\")\n","        print(f\"   Success:  {summary['success']} | Failures: {summary['failure']}\")\n","\n","\n","# COMMAND ----------\n","\n","# RUN PIPELINE\n","run_production_etl()\n","\n","# COMMAND ----------\n","# MAGIC %md\n","# MAGIC ## VERIFICATION\n","\n","# COMMAND ----------\n","\n","print(\"\\nüìä TABLE RECORD COUNTS (this run):\")\n","print(f\"   {'Table':<40} {'Records':>15}   Notes\")\n","print(\"   \" + \"-\" * 72)\n","\n","tables = spark.sql(f\"SHOW TABLES IN {Config.DATABASE}\").filter(\n","    F.col(\"tableName\").like(\"%person%\") |\n","    F.col(\"tableName\").like(\"%audit%\")  |\n","    F.col(\"tableName\").like(\"%rca%\")\n",").collect()\n","\n","for t in tables:\n","    full_name = f\"{Config.DATABASE}.{t.tableName}\"\n","    count     = spark.table(full_name).count()\n","    note      = \"\"\n","    if t.tableName == \"dim_person\":\n","        current = spark.table(full_name).filter(F.col(\"is_current\") == True).count()\n","        expired = count - current\n","        note    = f\"({current:,} current | {expired:,} expired)\"\n","    elif t.tableName == \"bronze_person\":\n","        note    = \"(should match source count ‚Äî watch for drift)\"\n","    print(f\"   {t.tableName:<40} {count:>15,}   {note}\")\n","\n","print(\"\\nüìã Schema change history ‚Äî from audit_trail:\")\n","display(\n","    spark.table(Config.table(\"audit_trail\"))\n","         .filter(F.col(\"event_type\").like(\"SCHEMA%\"))\n","         .orderBy(F.col(\"timestamp\").desc())\n","         .select(\"timestamp\", \"event_type\", \"status\", \"description\", \"session_id\")\n",")\n","\n","# COMMAND ----------\n","# MAGIC %md\n","# MAGIC ---\n","# MAGIC ## ETL v3.2 ‚Äî Change Summary\n","# MAGIC\n","# MAGIC **Single change in this increment ‚Äî audit_trail rows type conflict:**\n","# MAGIC\n","# MAGIC | | v3.1 (failed) | v3.2 (correct) |\n","# MAGIC |---|---|---|\n","# MAGIC | Approach | ALTER TABLE CHANGE COLUMN INT ‚Üí BIGINT | Detect existing type at runtime, align outgoing schema to match |\n","# MAGIC | Why v3.1 failed | ALTER TABLE CHANGE COLUMN unsupported in this Delta protocol version | N/A |\n","# MAGIC | Why mergeSchema failed | mergeSchema handles additive changes only ‚Äî not type conflicts | N/A |\n","# MAGIC | Fallback | Overwrote audit_trail ‚Äî destroyed history | Not reachable under normal operations |\n","# MAGIC | rows type in table | INT (established in v1, cannot be widened via DDL here) | INT ‚Äî matched at write time |\n","# MAGIC | rows type in DataFrame | LongType (v2/v3 declaration) | Dynamically matched to existing table |\n","# MAGIC | Overflow risk | At 2.1B rows | Same ‚Äî planned FORCE_RECREATE is the upgrade path at scale |\n","# MAGIC\n","# MAGIC **Bronze table strategy ‚Äî on record:**\n","# MAGIC The MERGE on `person_id` is the correct pattern for a full-extract source.\n","# MAGIC Source deletions (records absent from the current extract) are not yet handled.\n","# MAGIC Orphaned Bronze rows from deleted source records accumulate silently.\n","# MAGIC This is tracked as a pending item for v4.0 (soft-delete via `is_deleted` flag).\n","# MAGIC\n","# MAGIC **Pending ‚Äî next iterations:**\n","# MAGIC - v3.3: DQ reads from persisted bronze table + expand NHS ECDS rule set + apply_nhs_rules() error handling\n","# MAGIC - v4.0: Source deletion handling in Bronze, quarantine retention, location context, idempotency guard\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","spark_jobs_updating":false,"session_id":"eb6b5536-2ea7-457e-867b-ffe1dc16b305","normalized_state":"finished","queued_time":"2026-02-27T21:48:20.1152059Z","session_start_time":"2026-02-27T21:48:20.1181356Z","execution_start_time":"2026-02-27T21:48:32.4514147Z","execution_finish_time":"2026-02-27T21:55:27.6295456Z","parent_msg_id":"7226ad5b-a917-46e4-bacf-6f501b26d485"},"text/plain":"StatementMeta(, eb6b5536-2ea7-457e-867b-ffe1dc16b305, 3, Finished, Available, Finished, False)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["================================================================================\nPRODUCTION ETL - v3.0\n================================================================================\nSpark:    3.5.5.5.4.20260109.1\nDatabase: chimcobldhq2al3id5gmo9acc5lmachk4li64ro\n================================================================================\nConfig: Partitions=400 | FORCE_RECREATE=False\nPipeline: person_etl_v3 | Env: PROD\n‚úÖ Utilities loaded\n\n================================================================================\nPRODUCTION ETL v3.2 ‚Äî EXECUTION\n================================================================================\nSession:       88105352-6749-46e8-878a-87333e00a5d5\nPipeline:      person_etl_v3\nEnvironment:   PROD\nForceRecreate: False\n================================================================================\n‚úÖ PIPELINE_START: ETL v3.2 started | person_etl_v3\n\n[BRONZE] Raw ingestion...\n‚úÖ BRONZE_LOADED: Loaded 2,712,818 records from person\n‚úÖ SCHEMA_CHECK: Schema compatible ‚Äî no changes for dbo.bronze_person\n‚úÖ BRONZE_MERGE: Merged into dbo.bronze_person\n‚úÖ BRONZE_COMPLETE: Bronze complete: 2,712,818 records in 88.98s (30488 rows/s)\n\n[SILVER] Validation & enrichment...\n‚úÖ DQ_VALIDATION: Pass rate: 100.0%\n   DQ Pass Rate: 100.0%\n   Valid: 2,712,818 | Quarantine: 0\n‚úÖ PSEUDONYMIZATION: Applied GDPR pseudonymization\n‚úÖ SCHEMA_CHECK: Schema compatible ‚Äî no changes for dbo.silver_person\n‚úÖ SILVER_COMPLETE: Silver complete: 2,712,818 records\n\n[GOLD] Business layer...\n‚úÖ GOLD_KEY: person_key sourced from: person_source_value_pseudo\n‚úÖ SCHEMA_CHECK: Schema compatible ‚Äî no changes for dbo.gold_person\n‚úÖ GOLD_COMPLETE: Gold complete: 2,712,818 records\n\n[DIM] Dimension (SCD Type 2)...\n‚úÖ SCHEMA_CHECK: Schema compatible ‚Äî no changes for dbo.dim_person\n‚úÖ DIM_SCD2_EXPIRE: Step 1: expired changed current records\n‚úÖ DIM_SCD2_INSERT: Step 2: inserted new current versions\n‚úÖ DIM_COMPLETE: Dimension complete: 2,724,812 total | 2,712,818 current | 11,994 expired\n\n================================================================================\n‚úÖ‚úÖ‚úÖ PIPELINE SUCCESS ‚úÖ‚úÖ‚úÖ\n================================================================================\nSession:     88105352-6749-46e8-878a-87333e00a5d5\nDuration:    387.98s\nBronze:      2,712,818\nSilver:      2,712,818 (DQ: 100.0%)\nGold:        2,712,818\nDimension:   2,724,812 total | 2,712,818 current | 11,994 expired\nThroughput:  6,992 rows/s\nNHS ECDS:    v3.0 ‚úÖ | GDPR: Pseudonymized ‚úÖ\nActions:     {'bronze_person': 'MERGE', 'silver_person': 'MERGE', 'gold_person': 'MERGE', 'dim_person': 'MERGE'}\n================================================================================\n‚úÖ PIPELINE_COMPLETE: Pipeline completed successfully\n\nüìä Session Summary:\n   Duration: 399.65s\n   Events:   17\n   Success:  17 | Failures: 0\n\nüìä TABLE RECORD COUNTS (this run):\n   Table                                            Records   Notes\n   ------------------------------------------------------------------------\n   audit_processing_logs                                 35   \n   audit_trail                                           51   \n   bronze_person                                  2,712,818   (should match source count ‚Äî watch for drift)\n   dim_person                                     2,724,812   (2,712,818 current | 11,994 expired)\n   gold_person                                    2,712,818   \n   person                                         2,712,818   \n   rca_errors                                             1   \n   silver_person                                  2,712,818   \n\nüìã Schema change history ‚Äî from audit_trail:\n"]},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"0e51391a-33fd-4ca5-af53-461793ced782","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 0e51391a-33fd-4ca5-af53-461793ced782)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"advisor":{"adviceMetadata":"{\"artifactId\":\"d35d1b2e-14c8-453c-abc5-e3e2331e5627\",\"activityId\":\"eb6b5536-2ea7-457e-867b-ffe1dc16b305\",\"applicationId\":\"application_1772228250259_0001\",\"jobGroupId\":\"3\",\"advices\":{\"warn\":18,\"error\":1}}"},"collapsed":false},"id":"06591261-4811-4264-9f42-4eaca27c8b19"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{"0e51391a-33fd-4ca5-af53-461793ced782":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"2026-02-27 21:53:26.74802","1":"SCHEMA_CHECK","2":"SUCCESS","3":"Schema compatible ‚Äî no changes for dbo.dim_person","4":"88105352-6749-46e8-878a-87333e00a5d5","key":0,"index":0},{"0":"2026-02-27 21:52:18.101383","1":"SCHEMA_CHECK","2":"SUCCESS","3":"Schema compatible ‚Äî no changes for dbo.gold_person","4":"88105352-6749-46e8-878a-87333e00a5d5","key":1,"index":1},{"0":"2026-02-27 21:50:13.301792","1":"SCHEMA_CHECK","2":"SUCCESS","3":"Schema compatible ‚Äî no changes for dbo.silver_person","4":"88105352-6749-46e8-878a-87333e00a5d5","key":2,"index":2},{"0":"2026-02-27 21:48:57.48415","1":"SCHEMA_CHECK","2":"SUCCESS","3":"Schema compatible ‚Äî no changes for dbo.bronze_person","4":"88105352-6749-46e8-878a-87333e00a5d5","key":3,"index":3},{"0":"2026-02-18 16:48:30.364257","1":"SCHEMA_CHECK","2":"SUCCESS","3":"Schema compatible ‚Äî no changes for dbo.dim_person","4":"1338fa09-d29d-437a-b7f2-9b9d6f0e7317","key":4,"index":4},{"0":"2026-02-18 16:47:43.776427","1":"SCHEMA_CHECK","2":"SUCCESS","3":"Schema compatible ‚Äî no changes for dbo.gold_person","4":"1338fa09-d29d-437a-b7f2-9b9d6f0e7317","key":5,"index":5},{"0":"2026-02-18 16:46:47.826991","1":"SCHEMA_CHECK","2":"SUCCESS","3":"Schema compatible ‚Äî no changes for dbo.silver_person","4":"1338fa09-d29d-437a-b7f2-9b9d6f0e7317","key":6,"index":6},{"0":"2026-02-18 16:46:00.282946","1":"SCHEMA_CHECK","2":"SUCCESS","3":"Schema compatible ‚Äî no changes for dbo.bronze_person","4":"1338fa09-d29d-437a-b7f2-9b9d6f0e7317","key":7,"index":7},{"0":"2026-02-18 16:27:24.709212","1":"SCHEMA_CHECK","2":"SUCCESS","3":"Schema compatible ‚Äî no changes for dbo.dim_person","4":"0a18d9a3-a655-40d4-8c7d-ee9d00b46424","key":8,"index":8},{"0":"2026-02-18 16:25:27.870042","1":"SCHEMA_CHECK","2":"SUCCESS","3":"Schema compatible ‚Äî no changes for dbo.gold_person","4":"0a18d9a3-a655-40d4-8c7d-ee9d00b46424","key":9,"index":9},{"0":"2026-02-18 16:24:13.258564","1":"SCHEMA_CHECK","2":"SUCCESS","3":"Schema compatible ‚Äî no changes for dbo.silver_person","4":"0a18d9a3-a655-40d4-8c7d-ee9d00b46424","key":10,"index":10},{"0":"2026-02-18 16:23:08.166183","1":"SCHEMA_CHECK","2":"SUCCESS","3":"Schema compatible ‚Äî no changes for dbo.bronze_person","4":"0a18d9a3-a655-40d4-8c7d-ee9d00b46424","key":11,"index":11}],"schema":[{"key":"0","name":"timestamp","type":"timestamp"},{"key":"1","name":"event_type","type":"string"},{"key":"2","name":"status","type":"string"},{"key":"3","name":"description","type":"string"},{"key":"4","name":"session_id","type":"string"}],"truncated":false},"isSummary":false,"language":"scala","wranglerEntryContext":{"dataframeType":"pyspark"}},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":[],"seriesFieldKeys":[],"aggregationType":"sum","isStacked":false,"binsNumber":10,"wordFrequency":"-1"},"viewOptionsGroup":[{"tabItems":[{"type":"table","name":"Table","key":"0","options":{}}]}]}}}}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"5d277b6d-26ce-4249-a7d9-a6eac8face79"}],"default_lakehouse":"5d277b6d-26ce-4249-a7d9-a6eac8face79","default_lakehouse_name":"Lake24","default_lakehouse_workspace_id":"591f29e6-d45a-4989-9459-a5a7bf1b39b8"}}},"nbformat":4,"nbformat_minor":5}