{"cells":[{"cell_type":"code","source":["# Databricks notebook source\n","# MAGIC %md\n","# MAGIC # PHASE 1 - STEP 3: ETL CONTROL TABLE (WATERMARK MANAGEMENT)\n","# MAGIC \n","# MAGIC **Purpose:** Create control table for incremental load pattern\n","# MAGIC \n","# MAGIC **What This Does:**\n","# MAGIC - Creates `etl_control` table to track last processed timestamp per table/layer\n","# MAGIC - Provides watermark management functions (get/update/history)\n","# MAGIC - Initializes watermarks for person table across all layers\n","# MAGIC - Enables 99% runtime reduction in Step 4\n","# MAGIC \n","# MAGIC **Why This Matters:**\n","# MAGIC - Current: ETL processes ALL 16.7M records every run (469 seconds)\n","# MAGIC - With watermarks: ETL processes only NEW records (~1-5K per day, ~5 seconds)\n","# MAGIC - Foundation for incremental load pattern\n","# MAGIC \n","# MAGIC **Dependencies:**\n","# MAGIC - ‚úÖ person table with audit columns (Step 1)\n","# MAGIC - ‚úÖ Synthetic generator v2.0 creating timestamped records (Step 2)\n","# MAGIC - üü° ETL v4.0 will use this table (Step 4)\n","# MAGIC \n","# MAGIC **Impact:**\n","# MAGIC - ‚úÖ Zero impact on existing ETL v3.2 (still runs unchanged)\n","# MAGIC - ‚úÖ Non-destructive (only creates new table)\n","# MAGIC - ‚úÖ Read-only for now (Step 4 will update watermarks)\n","\n","# COMMAND ----------\n","\n","import pyspark.sql.functions as F\n","from pyspark.sql.types import *\n","from datetime import datetime\n","import json\n","\n","print(\"=\" * 80)\n","print(\"PHASE 1 - STEP 3: ETL CONTROL TABLE\")\n","print(\"=\" * 80)\n","print(f\"Spark Version: {spark.version}\")\n","print(f\"Execution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","print(f\"Database: {spark.sql('SELECT current_database()').collect()[0][0]}\")\n","print(\"=\" * 80)\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## CONFIGURATION\n","\n","# COMMAND ----------\n","\n","class Config:\n","    \"\"\"Configuration for ETL Control Table\"\"\"\n","    DATABASE = \"dbo\"\n","    CONTROL_TABLE = \"etl_control\"\n","    \n","    # Tables to track (add more as needed)\n","    TRACKED_TABLES = [\n","        {\n","            \"table_name\": \"person\",\n","            \"layers\": [\"BRONZE\", \"SILVER\", \"GOLD\", \"DIM\"],\n","            \"watermark_column\": \"updated_timestamp\",  # Column to use for watermark\n","            \"description\": \"Person master data table\"\n","        }\n","        # Add more tables here as your pipeline grows\n","        # {\n","        #     \"table_name\": \"encounter\",\n","        #     \"layers\": [\"BRONZE\", \"SILVER\", \"GOLD\"],\n","        #     \"watermark_column\": \"encounter_timestamp\",\n","        #     \"description\": \"Patient encounters\"\n","        # }\n","    ]\n","    \n","    @staticmethod\n","    def table(name):\n","        return f\"{Config.DATABASE}.{name}\"\n","\n","print(f\"üìã CONFIGURATION:\")\n","print(f\"   Database: {Config.DATABASE}\")\n","print(f\"   Control Table: {Config.table(Config.CONTROL_TABLE)}\")\n","print(f\"   Tracked Tables: {len(Config.TRACKED_TABLES)}\")\n","for t in Config.TRACKED_TABLES:\n","    print(f\"      - {t['table_name']} ({len(t['layers'])} layers)\")\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## STEP 3.1: PRE-FLIGHT CHECKS\n","\n","# COMMAND ----------\n","\n","def preflight_checks():\n","    \"\"\"\n","    Safety checks before creating control table\n","    \n","    Verifies:\n","    1. Control table doesn't already exist (or can be recreated)\n","    2. Tracked tables exist and have watermark columns\n","    3. Database is accessible\n","    \n","    Returns: (success: bool, findings: dict)\n","    \"\"\"\n","    print(\"\\nüîç PRE-FLIGHT CHECKS:\")\n","    print(\"-\" * 80)\n","    \n","    findings = {\n","        \"control_table_exists\": False,\n","        \"tracked_tables_valid\": [],\n","        \"warnings\": [],\n","        \"errors\": []\n","    }\n","    \n","    try:\n","        # 1. Check if control table already exists\n","        control_table_full = Config.table(Config.CONTROL_TABLE)\n","        if spark.catalog.tableExists(control_table_full):\n","            findings[\"control_table_exists\"] = True\n","            existing_count = spark.table(control_table_full).count()\n","            print(f\"   ‚ö†Ô∏è  Control table already exists: {control_table_full}\")\n","            print(f\"      Existing records: {existing_count:,}\")\n","            print(f\"      Will DROP and recreate (backup recommended)\")\n","            findings[\"warnings\"].append(f\"Control table exists with {existing_count} records\")\n","        else:\n","            print(f\"   ‚úÖ Control table does not exist (will create)\")\n","        \n","        # 2. Verify tracked tables and their watermark columns\n","        print(f\"\\n   üìä TRACKED TABLE VALIDATION:\")\n","        \n","        for table_config in Config.TRACKED_TABLES:\n","            table_name = table_config[\"table_name\"]\n","            watermark_col = table_config[\"watermark_column\"]\n","            table_full = Config.table(table_name)\n","            \n","            validation = {\n","                \"table_name\": table_name,\n","                \"exists\": False,\n","                \"has_watermark_column\": False,\n","                \"record_count\": 0,\n","                \"watermark_column\": watermark_col,\n","                \"sample_watermark_values\": {}\n","            }\n","            \n","            # Check if table exists\n","            if not spark.catalog.tableExists(table_full):\n","                print(f\"      ‚ùå {table_name}: Table does not exist!\")\n","                findings[\"errors\"].append(f\"Table {table_name} not found\")\n","                validation[\"exists\"] = False\n","                findings[\"tracked_tables_valid\"].append(validation)\n","                continue\n","            \n","            validation[\"exists\"] = True\n","            \n","            # Get table schema and count\n","            table_df = spark.table(table_full)\n","            validation[\"record_count\"] = table_df.count()\n","            table_columns = table_df.columns\n","            \n","            # Check if watermark column exists\n","            if watermark_col not in table_columns:\n","                print(f\"      ‚ùå {table_name}: Missing watermark column '{watermark_col}'\")\n","                findings[\"errors\"].append(f\"Table {table_name} missing column {watermark_col}\")\n","                validation[\"has_watermark_column\"] = False\n","            else:\n","                validation[\"has_watermark_column\"] = True\n","                \n","                # Get sample watermark values\n","                watermark_stats = table_df.select(\n","                    F.min(watermark_col).alias(\"min_watermark\"),\n","                    F.max(watermark_col).alias(\"max_watermark\"),\n","                    F.count(F.when(F.col(watermark_col).isNotNull(), 1)).alias(\"non_null_count\"),\n","                    F.count(\"*\").alias(\"total_count\")\n","                ).collect()[0]\n","                \n","                validation[\"sample_watermark_values\"] = {\n","                    \"min\": str(watermark_stats[\"min_watermark\"]),\n","                    \"max\": str(watermark_stats[\"max_watermark\"]),\n","                    \"non_null\": watermark_stats[\"non_null_count\"],\n","                    \"total\": watermark_stats[\"total_count\"]\n","                }\n","                \n","                non_null_pct = (watermark_stats[\"non_null_count\"] / watermark_stats[\"total_count\"] * 100) if watermark_stats[\"total_count\"] > 0 else 0\n","                \n","                print(f\"      ‚úÖ {table_name}:\")\n","                print(f\"         Records: {validation['record_count']:,}\")\n","                print(f\"         Watermark column: {watermark_col}\")\n","                print(f\"         Non-null watermarks: {watermark_stats['non_null_count']:,} ({non_null_pct:.1f}%)\")\n","                print(f\"         Min: {watermark_stats['min_watermark']}\")\n","                print(f\"         Max: {watermark_stats['max_watermark']}\")\n","                \n","                # Warning if many NULLs (expected for historical data)\n","                if non_null_pct < 10:\n","                    print(f\"         ‚ö†Ô∏è  Note: {100-non_null_pct:.1f}% NULL watermarks (historical data)\")\n","                    findings[\"warnings\"].append(f\"{table_name}: {100-non_null_pct:.1f}% NULL watermarks\")\n","            \n","            findings[\"tracked_tables_valid\"].append(validation)\n","        \n","        # 3. Summary\n","        print(f\"\\n   üìã SUMMARY:\")\n","        valid_tables = sum(1 for t in findings[\"tracked_tables_valid\"] if t[\"exists\"] and t[\"has_watermark_column\"])\n","        print(f\"      Valid tables: {valid_tables}/{len(Config.TRACKED_TABLES)}\")\n","        print(f\"      Warnings: {len(findings['warnings'])}\")\n","        print(f\"      Errors: {len(findings['errors'])}\")\n","        \n","        if findings[\"errors\"]:\n","            print(f\"\\n   ‚ùå PRE-FLIGHT CHECKS FAILED\")\n","            for error in findings[\"errors\"]:\n","                print(f\"      - {error}\")\n","            print(\"-\" * 80)\n","            return False, findings\n","        \n","        print(f\"\\n   ‚úÖ PRE-FLIGHT CHECKS PASSED\")\n","        if findings[\"warnings\"]:\n","            print(f\"   ‚ö†Ô∏è  {len(findings['warnings'])} warning(s) - review above\")\n","        \n","    except Exception as e:\n","        print(f\"   ‚ùå PRE-FLIGHT CHECK ERROR: {str(e)}\")\n","        findings[\"errors\"].append(str(e))\n","        return False, findings\n","    \n","    print(\"-\" * 80)\n","    return True, findings\n","\n","# Run pre-flight checks\n","checks_passed, findings = preflight_checks()\n","\n","if not checks_passed:\n","    raise Exception(\"Pre-flight checks failed. Review errors above.\")\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## STEP 3.2: CREATE ETL CONTROL TABLE\n","# MAGIC \n","# MAGIC **Schema Design:**\n","# MAGIC \n","# MAGIC ```\n","# MAGIC etl_control (\n","# MAGIC     control_id          STRING PRIMARY KEY  -- Unique ID for each record\n","# MAGIC     table_name          STRING NOT NULL     -- Source table name (e.g., 'person')\n","# MAGIC     layer               STRING NOT NULL     -- Pipeline layer (BRONZE/SILVER/GOLD/DIM)\n","# MAGIC     last_watermark      TIMESTAMP           -- Last processed timestamp\n","# MAGIC     last_run_time       TIMESTAMP           -- When ETL last ran\n","# MAGIC     rows_processed      BIGINT              -- Records processed in last run\n","# MAGIC     rows_quarantined    BIGINT              -- Records quarantined in last run\n","# MAGIC     status              STRING              -- INITIALIZED/RUNNING/SUCCESS/FAILED\n","# MAGIC     session_id          STRING              -- ETL session ID (from audit_trail)\n","# MAGIC     error_message       STRING              -- Error if status=FAILED\n","# MAGIC     metadata            STRING              -- JSON metadata (flexible extension)\n","# MAGIC     created_date        TIMESTAMP           -- When record was created\n","# MAGIC     updated_date        TIMESTAMP           -- When record was last updated\n","# MAGIC )\n","# MAGIC ```\n","# MAGIC \n","# MAGIC **Why This Schema:**\n","# MAGIC - `table_name + layer`: Unique key (e.g., person + BRONZE)\n","# MAGIC - `last_watermark`: Critical for incremental load (e.g., \"2026-03-01 07:24:04\")\n","# MAGIC - `last_run_time`: When ETL ran (for monitoring)\n","# MAGIC - `rows_processed`: Track volume (for SLA monitoring)\n","# MAGIC - `status`: Track ETL state (RUNNING prevents concurrent runs)\n","# MAGIC - `session_id`: Link to audit_trail for full lineage\n","# MAGIC - `metadata`: JSON for flexibility (custom config per table)\n","\n","# COMMAND ----------\n","\n","def create_control_table():\n","    \"\"\"\n","    Create etl_control table with proper schema\n","    \n","    DAMA Best Practice: Control tables are critical metadata\n","    - Must be reliable (use Delta Lake for ACID)\n","    - Must be auditable (include created/updated timestamps)\n","    - Must be flexible (metadata JSON column)\n","    \n","    Returns: (success: bool, message: str)\n","    \"\"\"\n","    print(\"\\nüîß CREATING ETL CONTROL TABLE:\")\n","    print(\"-\" * 80)\n","    \n","    control_table_full = Config.table(Config.CONTROL_TABLE)\n","    \n","    try:\n","        # 1. Drop if exists (based on pre-flight check)\n","        if findings[\"control_table_exists\"]:\n","            print(f\"   ‚ö†Ô∏è  Dropping existing table: {control_table_full}\")\n","            spark.sql(f\"DROP TABLE IF EXISTS {control_table_full}\")\n","            print(f\"   ‚úÖ Dropped successfully\")\n","        \n","        # 2. Define schema\n","        schema = StructType([\n","            StructField(\"control_id\", StringType(), False),          # PK\n","            StructField(\"table_name\", StringType(), False),          # Source table\n","            StructField(\"layer\", StringType(), False),               # BRONZE/SILVER/GOLD/DIM\n","            StructField(\"last_watermark\", TimestampType(), True),    # Last processed timestamp\n","            StructField(\"last_run_time\", TimestampType(), True),     # When ETL ran\n","            StructField(\"rows_processed\", LongType(), True),         # Records processed\n","            StructField(\"rows_quarantined\", LongType(), True),       # Records quarantined\n","            StructField(\"status\", StringType(), True),               # INITIALIZED/RUNNING/SUCCESS/FAILED\n","            StructField(\"session_id\", StringType(), True),           # Link to audit_trail\n","            StructField(\"error_message\", StringType(), True),        # Error details if FAILED\n","            StructField(\"metadata\", StringType(), True),             # JSON for flexibility\n","            StructField(\"created_date\", TimestampType(), False),     # Audit: when created\n","            StructField(\"updated_date\", TimestampType(), False)      # Audit: when updated\n","        ])\n","        \n","        # 3. Create empty dataframe\n","        empty_df = spark.createDataFrame([], schema)\n","        \n","        # 4. Write as Delta table\n","        print(f\"   üìù Creating table: {control_table_full}\")\n","        empty_df.write \\\n","            .format(\"delta\") \\\n","            .mode(\"overwrite\") \\\n","            .option(\"overwriteSchema\", \"true\") \\\n","            .saveAsTable(control_table_full)\n","        \n","        # 5. Verify creation\n","        if spark.catalog.tableExists(control_table_full):\n","            print(f\"   ‚úÖ Table created successfully\")\n","            \n","            # Show schema\n","            print(f\"\\n   üìä TABLE SCHEMA:\")\n","            schema_df = spark.sql(f\"DESCRIBE TABLE {control_table_full}\")\n","            schema_df.show(truncate=False)\n","            \n","            return True, \"Table created successfully\"\n","        else:\n","            return False, \"Table creation failed - table does not exist after write\"\n","        \n","    except Exception as e:\n","        print(f\"   ‚ùå ERROR: {str(e)}\")\n","        return False, str(e)\n","    \n","    print(\"-\" * 80)\n","\n","# Create the table\n","create_success, create_message = create_control_table()\n","\n","if not create_success:\n","    raise Exception(f\"Control table creation failed: {create_message}\")\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## STEP 3.3: INITIALIZE WATERMARKS\n","# MAGIC \n","# MAGIC **What This Does:**\n","# MAGIC - Creates one control record per table per layer\n","# MAGIC - Sets `last_watermark = NULL` (will process ALL data on first run)\n","# MAGIC - Sets `status = INITIALIZED`\n","# MAGIC - Populates metadata with table configuration\n","# MAGIC \n","# MAGIC **Why NULL Watermark:**\n","# MAGIC - First run: Process ALL existing data (create baseline)\n","# MAGIC - Subsequent runs: Process only data AFTER last watermark\n","# MAGIC - This enables transition from full load ‚Üí incremental load\n","\n","# COMMAND ----------\n","\n","def initialize_watermarks():\n","    \"\"\"\n","    Initialize watermark records for all tracked tables\n","    \n","    Creates one record per table per layer with:\n","    - last_watermark = NULL (first run will be full load)\n","    - status = INITIALIZED\n","    - metadata = table configuration\n","    \n","    Returns: (success: bool, records_created: int)\n","    \"\"\"\n","    print(\"\\nüîß INITIALIZING WATERMARKS:\")\n","    print(\"-\" * 80)\n","    \n","    control_table_full = Config.table(Config.CONTROL_TABLE)\n","    records_to_insert = []\n","    current_ts = datetime.now()\n","    \n","    try:\n","        # Generate control records\n","        for table_config in Config.TRACKED_TABLES:\n","            table_name = table_config[\"table_name\"]\n","            layers = table_config[\"layers\"]\n","            watermark_col = table_config[\"watermark_column\"]\n","            description = table_config.get(\"description\", \"\")\n","            \n","            print(f\"   üìù {table_name}:\")\n","            \n","            for layer in layers:\n","                control_id = f\"{table_name}_{layer}\"\n","                \n","                metadata = {\n","                    \"watermark_column\": watermark_col,\n","                    \"description\": description,\n","                    \"initialized_by\": \"Phase1_Step3\",\n","                    \"initialization_date\": current_ts.isoformat(),\n","                    \"source_table_full\": Config.table(table_name),\n","                    \"target_table_full\": Config.table(f\"{layer.lower()}_{table_name}\")\n","                }\n","                \n","                record = (\n","                    control_id,                     # control_id\n","                    table_name,                     # table_name\n","                    layer,                          # layer\n","                    None,                           # last_watermark (NULL = full load on first run)\n","                    None,                           # last_run_time\n","                    0,                              # rows_processed\n","                    0,                              # rows_quarantined\n","                    \"INITIALIZED\",                  # status\n","                    None,                           # session_id\n","                    None,                           # error_message\n","                    json.dumps(metadata),           # metadata (JSON)\n","                    current_ts,                     # created_date\n","                    current_ts                      # updated_date\n","                )\n","                \n","                records_to_insert.append(record)\n","                print(f\"      ‚úÖ {layer}: {control_id}\")\n","        \n","        # Create DataFrame\n","        print(f\"\\n   üíæ INSERTING RECORDS:\")\n","        schema = spark.table(control_table_full).schema\n","        control_df = spark.createDataFrame(records_to_insert, schema)\n","        \n","        # Show what we're inserting\n","        print(f\"      Records to insert: {len(records_to_insert)}\")\n","        control_df.select(\"control_id\", \"table_name\", \"layer\", \"status\").show(truncate=False)\n","        \n","        # Insert into control table\n","        control_df.write \\\n","            .format(\"delta\") \\\n","            .mode(\"append\") \\\n","            .saveAsTable(control_table_full)\n","        \n","        # Verify insertion\n","        inserted_count = spark.table(control_table_full).count()\n","        print(f\"\\n   ‚úÖ Initialization complete\")\n","        print(f\"      Records inserted: {len(records_to_insert)}\")\n","        print(f\"      Total in table: {inserted_count}\")\n","        \n","        return True, len(records_to_insert)\n","        \n","    except Exception as e:\n","        print(f\"   ‚ùå ERROR: {str(e)}\")\n","        return False, 0\n","    \n","    print(\"-\" * 80)\n","\n","# Initialize watermarks\n","init_success, records_created = initialize_watermarks()\n","\n","if not init_success:\n","    raise Exception(\"Watermark initialization failed\")\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## STEP 3.4: HELPER FUNCTIONS\n","# MAGIC \n","# MAGIC **Purpose:** Provide reusable functions for watermark management\n","# MAGIC \n","# MAGIC **Functions:**\n","# MAGIC 1. `get_last_watermark(table_name, layer)` - Get last processed timestamp\n","# MAGIC 2. `update_watermark(...)` - Update after ETL run\n","# MAGIC 3. `get_watermark_history(...)` - Query history for monitoring\n","# MAGIC 4. `get_tables_to_process()` - List tables ready for processing\n","# MAGIC \n","# MAGIC **Usage in Step 4 (ETL v4.0):**\n","# MAGIC ```python\n","# MAGIC last_ts = get_last_watermark('person', 'BRONZE')\n","# MAGIC new_records = source.filter(F.col('updated_timestamp') > last_ts)\n","# MAGIC # ... process new_records ...\n","# MAGIC update_watermark('person', 'BRONZE', new_max_ts, row_count, session_id)\n","# MAGIC ```\n","\n","# COMMAND ----------\n","\n","def get_last_watermark(table_name: str, layer: str):\n","    \"\"\"\n","    Get the last processed watermark for a table/layer\n","    \n","    Args:\n","        table_name: Source table name (e.g., 'person')\n","        layer: Pipeline layer ('BRONZE', 'SILVER', 'GOLD', 'DIM')\n","    \n","    Returns:\n","        last_watermark (timestamp or None)\n","    \n","    Usage:\n","        last_ts = get_last_watermark('person', 'BRONZE')\n","        if last_ts is None:\n","            # First run - full load\n","        else:\n","            # Incremental - filter by last_ts\n","    \"\"\"\n","    try:\n","        control_table_full = Config.table(Config.CONTROL_TABLE)\n","        \n","        result = spark.table(control_table_full) \\\n","            .filter((F.col(\"table_name\") == table_name) & (F.col(\"layer\") == layer)) \\\n","            .select(\"last_watermark\") \\\n","            .collect()\n","        \n","        if result:\n","            return result[0][\"last_watermark\"]\n","        else:\n","            print(f\"‚ö†Ô∏è  WARNING: No watermark found for {table_name}/{layer}\")\n","            return None\n","    \n","    except Exception as e:\n","        print(f\"‚ùå ERROR getting watermark: {str(e)}\")\n","        return None\n","\n","\n","def update_watermark(table_name: str, layer: str, new_watermark, \n","                     rows_processed: int, rows_quarantined: int = 0,\n","                     session_id: str = None, status: str = \"SUCCESS\",\n","                     error_message: str = None):\n","    \"\"\"\n","    Update watermark after successful ETL run\n","    \n","    Args:\n","        table_name: Source table name\n","        layer: Pipeline layer\n","        new_watermark: New max timestamp processed\n","        rows_processed: Number of records processed\n","        rows_quarantined: Number of records quarantined (default 0)\n","        session_id: ETL session ID for audit trail linkage\n","        status: 'SUCCESS', 'FAILED', or 'RUNNING'\n","        error_message: Error details if status='FAILED'\n","    \n","    Returns:\n","        success (bool)\n","    \n","    Usage:\n","        update_watermark('person', 'BRONZE', \n","                        new_max_ts, 1000, 0, session_id, 'SUCCESS')\n","    \"\"\"\n","    try:\n","        control_table_full = Config.table(Config.CONTROL_TABLE)\n","        current_ts = datetime.now()\n","        \n","        # Build update statement\n","        from delta.tables import DeltaTable\n","        \n","        delta_table = DeltaTable.forName(spark, control_table_full)\n","        \n","        delta_table.update(\n","            condition = f\"table_name = '{table_name}' AND layer = '{layer}'\",\n","            set = {\n","                \"last_watermark\": F.lit(new_watermark).cast(TimestampType()),\n","                \"last_run_time\": F.lit(current_ts).cast(TimestampType()),\n","                \"rows_processed\": F.lit(rows_processed).cast(LongType()),\n","                \"rows_quarantined\": F.lit(rows_quarantined).cast(LongType()),\n","                \"status\": F.lit(status),\n","                \"session_id\": F.lit(session_id),\n","                \"error_message\": F.lit(error_message),\n","                \"updated_date\": F.lit(current_ts).cast(TimestampType())\n","            }\n","        )\n","        \n","        print(f\"‚úÖ Watermark updated: {table_name}/{layer} ‚Üí {new_watermark}\")\n","        return True\n","        \n","    except Exception as e:\n","        print(f\"‚ùå ERROR updating watermark: {str(e)}\")\n","        return False\n","\n","\n","def get_watermark_history(table_name: str = None, layer: str = None, limit: int = 10):\n","    \"\"\"\n","    Get watermark history for monitoring\n","    \n","    Args:\n","        table_name: Filter by table (optional)\n","        layer: Filter by layer (optional)\n","        limit: Number of records to return\n","    \n","    Returns:\n","        DataFrame with watermark history\n","    \n","    Usage:\n","        history = get_watermark_history('person', 'BRONZE', 5)\n","        history.show()\n","    \"\"\"\n","    try:\n","        control_table_full = Config.table(Config.CONTROL_TABLE)\n","        \n","        df = spark.table(control_table_full)\n","        \n","        if table_name:\n","            df = df.filter(F.col(\"table_name\") == table_name)\n","        \n","        if layer:\n","            df = df.filter(F.col(\"layer\") == layer)\n","        \n","        return df.select(\n","            \"control_id\",\n","            \"table_name\",\n","            \"layer\",\n","            \"last_watermark\",\n","            \"last_run_time\",\n","            \"rows_processed\",\n","            \"status\"\n","        ).orderBy(F.col(\"updated_date\").desc()).limit(limit)\n","        \n","    except Exception as e:\n","        print(f\"‚ùå ERROR getting history: {str(e)}\")\n","        return None\n","\n","\n","def get_tables_to_process():\n","    \"\"\"\n","    Get list of tables ready for processing\n","    \n","    Returns tables where status != 'RUNNING' (not locked)\n","    \n","    Returns:\n","        DataFrame with processable tables\n","    \n","    Usage:\n","        tables = get_tables_to_process()\n","        for row in tables.collect():\n","            process_table(row.table_name, row.layer)\n","    \"\"\"\n","    try:\n","        control_table_full = Config.table(Config.CONTROL_TABLE)\n","        \n","        return spark.table(control_table_full) \\\n","            .filter(F.col(\"status\") != \"RUNNING\") \\\n","            .select(\"table_name\", \"layer\", \"last_watermark\", \"status\") \\\n","            .orderBy(\"table_name\", \"layer\")\n","        \n","    except Exception as e:\n","        print(f\"‚ùå ERROR: {str(e)}\")\n","        return None\n","\n","print(\"‚úÖ Helper functions defined:\")\n","print(\"   - get_last_watermark(table_name, layer)\")\n","print(\"   - update_watermark(table_name, layer, new_watermark, rows, ...)\")\n","print(\"   - get_watermark_history(table_name, layer, limit)\")\n","print(\"   - get_tables_to_process()\")\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## STEP 3.5: TEST HELPER FUNCTIONS\n","\n","# COMMAND ----------\n","\n","print(\"\\nüß™ TESTING HELPER FUNCTIONS:\")\n","print(\"=\" * 80)\n","\n","# Test 1: Get last watermark (should be NULL for initialized records)\n","print(\"\\n1Ô∏è‚É£ TEST: get_last_watermark()\")\n","print(\"-\" * 80)\n","last_wm = get_last_watermark('person', 'BRONZE')\n","print(f\"   Result: {last_wm}\")\n","print(f\"   Expected: None (NULL - first run will be full load)\")\n","print(f\"   Status: {'‚úÖ PASS' if last_wm is None else '‚ùå FAIL'}\")\n","\n","# Test 2: Get watermark history\n","print(\"\\n2Ô∏è‚É£ TEST: get_watermark_history()\")\n","print(\"-\" * 80)\n","history = get_watermark_history('person')\n","if history:\n","    print(f\"   Records found: {history.count()}\")\n","    history.show(truncate=False)\n","    print(f\"   Status: ‚úÖ PASS\")\n","else:\n","    print(f\"   Status: ‚ùå FAIL\")\n","\n","# Test 3: Get tables to process\n","print(\"\\n3Ô∏è‚É£ TEST: get_tables_to_process()\")\n","print(\"-\" * 80)\n","tables = get_tables_to_process()\n","if tables:\n","    print(f\"   Tables ready for processing: {tables.count()}\")\n","    tables.show(truncate=False)\n","    print(f\"   Status: ‚úÖ PASS\")\n","else:\n","    print(f\"   Status: ‚ùå FAIL\")\n","\n","# Test 4: Update watermark (simulate an ETL run)\n","print(\"\\n4Ô∏è‚É£ TEST: update_watermark() [SIMULATION]\")\n","print(\"-\" * 80)\n","test_watermark = datetime(2026, 3, 1, 7, 24, 0)  # Simulate max timestamp from data\n","test_session_id = \"test_session_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","\n","print(f\"   Simulating ETL run...\")\n","print(f\"   Table: person / Layer: BRONZE\")\n","print(f\"   New watermark: {test_watermark}\")\n","print(f\"   Rows processed: 1000000\")\n","print(f\"   Session ID: {test_session_id}\")\n","\n","update_success = update_watermark(\n","    table_name='person',\n","    layer='BRONZE',\n","    new_watermark=test_watermark,\n","    rows_processed=1000000,\n","    rows_quarantined=0,\n","    session_id=test_session_id,\n","    status='SUCCESS'\n",")\n","\n","if update_success:\n","    # Verify update\n","    new_wm = get_last_watermark('person', 'BRONZE')\n","    print(f\"\\n   Verification:\")\n","    print(f\"   Old watermark: None\")\n","    print(f\"   New watermark: {new_wm}\")\n","    print(f\"   Status: {'‚úÖ PASS' if new_wm == test_watermark else '‚ùå FAIL'}\")\n","    \n","    # Show updated record\n","    print(f\"\\n   Updated control record:\")\n","    spark.table(Config.table(Config.CONTROL_TABLE)) \\\n","        .filter((F.col(\"table_name\") == \"person\") & (F.col(\"layer\") == \"BRONZE\")) \\\n","        .show(truncate=False, vertical=True)\n","else:\n","    print(f\"   Status: ‚ùå FAIL\")\n","\n","print(\"=\" * 80)\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## STEP 3.6: VERIFICATION & SUMMARY\n","\n","# COMMAND ----------\n","\n","print(\"\\nüìä FINAL VERIFICATION:\")\n","print(\"=\" * 80)\n","\n","control_table_full = Config.table(Config.CONTROL_TABLE)\n","\n","# 1. Count check\n","total_records = spark.table(control_table_full).count()\n","expected_records = sum(len(t[\"layers\"]) for t in Config.TRACKED_TABLES)\n","\n","print(f\"1Ô∏è‚É£ RECORD COUNT:\")\n","print(f\"   Expected: {expected_records}\")\n","print(f\"   Actual:   {total_records}\")\n","print(f\"   Status:   {'‚úÖ PASS' if total_records == expected_records else '‚ùå FAIL'}\")\n","\n","# 2. Schema check\n","print(f\"\\n2Ô∏è‚É£ SCHEMA:\")\n","control_columns = spark.table(control_table_full).columns\n","required_columns = ['control_id', 'table_name', 'layer', 'last_watermark', \n","                   'last_run_time', 'rows_processed', 'status']\n","missing_columns = [col for col in required_columns if col not in control_columns]\n","\n","print(f\"   Total columns: {len(control_columns)}\")\n","print(f\"   Required columns: {required_columns}\")\n","print(f\"   Missing columns: {missing_columns if missing_columns else 'None'}\")\n","print(f\"   Status: {'‚úÖ PASS' if not missing_columns else '‚ùå FAIL'}\")\n","\n","# 3. Status check\n","print(f\"\\n3Ô∏è‚É£ STATUS DISTRIBUTION:\")\n","status_dist = spark.table(control_table_full) \\\n","    .groupBy(\"status\") \\\n","    .count() \\\n","    .orderBy(\"status\")\n","status_dist.show()\n","\n","# 4. Full table display\n","print(f\"\\n4Ô∏è‚É£ COMPLETE CONTROL TABLE:\")\n","spark.table(control_table_full) \\\n","    .select(\"control_id\", \"table_name\", \"layer\", \"last_watermark\", \n","            \"rows_processed\", \"status\", \"updated_date\") \\\n","    .show(truncate=False)\n","\n","print(\"=\" * 80)\n","print(\"‚úÖ‚úÖ‚úÖ PHASE 1 - STEP 3 COMPLETE ‚úÖ‚úÖ‚úÖ\")\n","print(\"=\" * 80)\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ## SUMMARY & NEXT STEPS\n","\n","# COMMAND ----------\n","\n","print(\"\\nüìã STEP 3 SUMMARY:\")\n","print(\"=\" * 80)\n","print(\"PHASE 1 - STEP 3: ETL CONTROL TABLE\")\n","print(\"=\" * 80)\n","print(f\"Status: ‚úÖ COMPLETE\")\n","print(f\"\\n‚úÖ CREATED:\")\n","print(f\"   Table: {Config.table(Config.CONTROL_TABLE)}\")\n","print(f\"   Records: {total_records}\")\n","print(f\"   Tracked Tables: {len(Config.TRACKED_TABLES)}\")\n","print(f\"   Tracked Layers: {sum(len(t['layers']) for t in Config.TRACKED_TABLES)}\")\n","print(f\"\\n‚úÖ INITIALIZED:\")\n","for table_config in Config.TRACKED_TABLES:\n","    print(f\"   {table_config['table_name']}: {', '.join(table_config['layers'])}\")\n","print(f\"\\n‚úÖ HELPER FUNCTIONS:\")\n","print(f\"   - get_last_watermark(table_name, layer)\")\n","print(f\"   - update_watermark(...)\")\n","print(f\"   - get_watermark_history(...)\")\n","print(f\"   - get_tables_to_process()\")\n","print(f\"\\n‚úÖ TESTED:\")\n","print(f\"   - Watermark retrieval: ‚úÖ\")\n","print(f\"   - Watermark update: ‚úÖ\")\n","print(f\"   - History query: ‚úÖ\")\n","print(f\"   - Table listing: ‚úÖ\")\n","print(\"=\" * 80)\n","\n","print(f\"\\nüìå NEXT STEPS:\")\n","print(f\"1. ‚úÖ Step 1: Audit columns added\")\n","print(f\"2. ‚úÖ Step 2: Synthetic generator updated\")\n","print(f\"3. ‚úÖ Step 3: ETL control table created\")\n","print(f\"4. ‚è≠Ô∏è  Step 4: Implement incremental load in ETL v4.0\")\n","print(f\"\\nüéØ READY FOR STEP 4:\")\n","print(f\"   - Control table ready: ‚úÖ\")\n","print(f\"   - Watermarks initialized: ‚úÖ\")\n","print(f\"   - Helper functions available: ‚úÖ\")\n","print(f\"   - Test data with timestamps: ‚úÖ (1M records from Step 2)\")\n","print(f\"\\n‚è≠Ô∏è  Next: Modify ETL v3.2 ‚Üí v4.0 for incremental load\")\n","print(f\"   Expected result: 469 seconds ‚Üí ~5 seconds (99% faster)\")\n","print(\"=\" * 80)\n","\n","# COMMAND ----------\n","\n","# MAGIC %md\n","# MAGIC ---\n","# MAGIC ## ‚úÖ PHASE 1 - STEP 3 COMPLETE\n","# MAGIC \n","# MAGIC **What Was Created:**\n","# MAGIC - ‚úÖ `etl_control` table with proper schema\n","# MAGIC - ‚úÖ Watermarks initialized for person table (4 layers)\n","# MAGIC - ‚úÖ Helper functions for watermark management\n","# MAGIC - ‚úÖ Test simulation successful\n","# MAGIC \n","# MAGIC **Impact:**\n","# MAGIC - ‚úÖ Zero impact on existing ETL v3.2\n","# MAGIC - ‚úÖ Foundation ready for incremental load\n","# MAGIC - ‚úÖ All tests passed\n","# MAGIC \n","# MAGIC **Usage in Step 4:**\n","# MAGIC ```python\n","# MAGIC # In ETL v4.0:\n","# MAGIC last_ts = get_last_watermark('person', 'BRONZE')\n","# MAGIC \n","# MAGIC if last_ts is None:\n","# MAGIC     # First run: full load\n","# MAGIC     new_records = spark.table(\"person\")\n","# MAGIC else:\n","# MAGIC     # Incremental: only new records\n","# MAGIC     new_records = spark.table(\"person\") \\\n","# MAGIC         .filter(F.col(\"updated_timestamp\") > last_ts)\n","# MAGIC \n","# MAGIC # Process new_records...\n","# MAGIC max_ts = new_records.agg(F.max(\"updated_timestamp\")).collect()[0][0]\n","# MAGIC update_watermark('person', 'BRONZE', max_ts, count, session_id)\n","# MAGIC ```\n","# MAGIC \n","# MAGIC **Ready for Step 4!** üöÄ\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","spark_jobs_updating":false,"session_id":"b448c307-ceaa-44dd-bce2-0b0594651c52","normalized_state":"finished","queued_time":"2026-03-01T07:48:26.7700719Z","session_start_time":"2026-03-01T07:48:26.7725669Z","execution_start_time":"2026-03-01T07:48:37.5923082Z","execution_finish_time":"2026-03-01T07:49:27.7610424Z","parent_msg_id":"a54c5e3b-6a4b-429b-ba03-d126637b017e"},"text/plain":"StatementMeta(, b448c307-ceaa-44dd-bce2-0b0594651c52, 3, Finished, Available, Finished, False)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["================================================================================\nPHASE 1 - STEP 3: ETL CONTROL TABLE\n================================================================================\nSpark Version: 3.5.5.5.4.20260109.1\nExecution Time: 2026-03-01 07:48:37\nDatabase: chimcobldhq2al3id5gmo9acc5lmachk4li64ro\n================================================================================\nüìã CONFIGURATION:\n   Database: dbo\n   Control Table: dbo.etl_control\n   Tracked Tables: 1\n      - person (4 layers)\n\nüîç PRE-FLIGHT CHECKS:\n--------------------------------------------------------------------------------\n   ‚úÖ Control table does not exist (will create)\n\n   üìä TRACKED TABLE VALIDATION:\n      ‚úÖ person:\n         Records: 16,712,818\n         Watermark column: updated_timestamp\n         Non-null watermarks: 1,000,000 (6.0%)\n         Min: 2026-03-01 07:24:04.931232\n         Max: 2026-03-01 07:24:04.931232\n         ‚ö†Ô∏è  Note: 94.0% NULL watermarks (historical data)\n\n   üìã SUMMARY:\n      Valid tables: 1/1\n      Warnings: 1\n      Errors: 0\n\n   ‚úÖ PRE-FLIGHT CHECKS PASSED\n   ‚ö†Ô∏è  1 warning(s) - review above\n--------------------------------------------------------------------------------\n\nüîß CREATING ETL CONTROL TABLE:\n--------------------------------------------------------------------------------\n   üìù Creating table: dbo.etl_control\n   ‚úÖ Table created successfully\n\n   üìä TABLE SCHEMA:\n+----------------+---------+-------+\n|col_name        |data_type|comment|\n+----------------+---------+-------+\n|control_id      |string   |NULL   |\n|table_name      |string   |NULL   |\n|layer           |string   |NULL   |\n|last_watermark  |timestamp|NULL   |\n|last_run_time   |timestamp|NULL   |\n|rows_processed  |bigint   |NULL   |\n|rows_quarantined|bigint   |NULL   |\n|status          |string   |NULL   |\n|session_id      |string   |NULL   |\n|error_message   |string   |NULL   |\n|metadata        |string   |NULL   |\n|created_date    |timestamp|NULL   |\n|updated_date    |timestamp|NULL   |\n+----------------+---------+-------+\n\n\nüîß INITIALIZING WATERMARKS:\n--------------------------------------------------------------------------------\n   üìù person:\n      ‚úÖ BRONZE: person_BRONZE\n      ‚úÖ SILVER: person_SILVER\n      ‚úÖ GOLD: person_GOLD\n      ‚úÖ DIM: person_DIM\n\n   üíæ INSERTING RECORDS:\n      Records to insert: 4\n+-------------+----------+------+-----------+\n|control_id   |table_name|layer |status     |\n+-------------+----------+------+-----------+\n|person_BRONZE|person    |BRONZE|INITIALIZED|\n|person_SILVER|person    |SILVER|INITIALIZED|\n|person_GOLD  |person    |GOLD  |INITIALIZED|\n|person_DIM   |person    |DIM   |INITIALIZED|\n+-------------+----------+------+-----------+\n\n\n   ‚úÖ Initialization complete\n      Records inserted: 4\n      Total in table: 4\n‚úÖ Helper functions defined:\n   - get_last_watermark(table_name, layer)\n   - update_watermark(table_name, layer, new_watermark, rows, ...)\n   - get_watermark_history(table_name, layer, limit)\n   - get_tables_to_process()\n\nüß™ TESTING HELPER FUNCTIONS:\n================================================================================\n\n1Ô∏è‚É£ TEST: get_last_watermark()\n--------------------------------------------------------------------------------\n   Result: None\n   Expected: None (NULL - first run will be full load)\n   Status: ‚úÖ PASS\n\n2Ô∏è‚É£ TEST: get_watermark_history()\n--------------------------------------------------------------------------------\n   Records found: 4\n+-------------+----------+------+--------------+-------------+--------------+-----------+\n|control_id   |table_name|layer |last_watermark|last_run_time|rows_processed|status     |\n+-------------+----------+------+--------------+-------------+--------------+-----------+\n|person_DIM   |person    |DIM   |NULL          |NULL         |0             |INITIALIZED|\n|person_BRONZE|person    |BRONZE|NULL          |NULL         |0             |INITIALIZED|\n|person_GOLD  |person    |GOLD  |NULL          |NULL         |0             |INITIALIZED|\n|person_SILVER|person    |SILVER|NULL          |NULL         |0             |INITIALIZED|\n+-------------+----------+------+--------------+-------------+--------------+-----------+\n\n   Status: ‚úÖ PASS\n\n3Ô∏è‚É£ TEST: get_tables_to_process()\n--------------------------------------------------------------------------------\n   Tables ready for processing: 4\n+----------+------+--------------+-----------+\n|table_name|layer |last_watermark|status     |\n+----------+------+--------------+-----------+\n|person    |BRONZE|NULL          |INITIALIZED|\n|person    |DIM   |NULL          |INITIALIZED|\n|person    |GOLD  |NULL          |INITIALIZED|\n|person    |SILVER|NULL          |INITIALIZED|\n+----------+------+--------------+-----------+\n\n   Status: ‚úÖ PASS\n\n4Ô∏è‚É£ TEST: update_watermark() [SIMULATION]\n--------------------------------------------------------------------------------\n   Simulating ETL run...\n   Table: person / Layer: BRONZE\n   New watermark: 2026-03-01 07:24:00\n   Rows processed: 1000000\n   Session ID: test_session_20260301_074913\n‚úÖ Watermark updated: person/BRONZE ‚Üí 2026-03-01 07:24:00\n\n   Verification:\n   Old watermark: None\n   New watermark: 2026-03-01 07:24:00\n   Status: ‚úÖ PASS\n\n   Updated control record:\n-RECORD 0--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n control_id       | person_BRONZE                                                                                                                                                                                                                                            \n table_name       | person                                                                                                                                                                                                                                                   \n layer            | BRONZE                                                                                                                                                                                                                                                   \n last_watermark   | 2026-03-01 07:24:00                                                                                                                                                                                                                                      \n last_run_time    | 2026-03-01 07:49:13.787503                                                                                                                                                                                                                               \n rows_processed   | 1000000                                                                                                                                                                                                                                                  \n rows_quarantined | 0                                                                                                                                                                                                                                                        \n status           | SUCCESS                                                                                                                                                                                                                                                  \n session_id       | test_session_20260301_074913                                                                                                                                                                                                                             \n error_message    | NULL                                                                                                                                                                                                                                                     \n metadata         | {\"watermark_column\": \"updated_timestamp\", \"description\": \"Person master data table\", \"initialized_by\": \"Phase1_Step3\", \"initialization_date\": \"2026-03-01T07:49:02.018525\", \"source_table_full\": \"dbo.person\", \"target_table_full\": \"dbo.bronze_person\"} \n created_date     | 2026-03-01 07:49:02.018525                                                                                                                                                                                                                               \n updated_date     | 2026-03-01 07:49:13.787503                                                                                                                                                                                                                               \n\n================================================================================\n\nüìä FINAL VERIFICATION:\n================================================================================\n1Ô∏è‚É£ RECORD COUNT:\n   Expected: 4\n   Actual:   4\n   Status:   ‚úÖ PASS\n\n2Ô∏è‚É£ SCHEMA:\n   Total columns: 13\n   Required columns: ['control_id', 'table_name', 'layer', 'last_watermark', 'last_run_time', 'rows_processed', 'status']\n   Missing columns: None\n   Status: ‚úÖ PASS\n\n3Ô∏è‚É£ STATUS DISTRIBUTION:\n+-----------+-----+\n|     status|count|\n+-----------+-----+\n|INITIALIZED|    3|\n|    SUCCESS|    1|\n+-----------+-----+\n\n\n4Ô∏è‚É£ COMPLETE CONTROL TABLE:\n+-------------+----------+------+-------------------+--------------+-----------+--------------------------+\n|control_id   |table_name|layer |last_watermark     |rows_processed|status     |updated_date              |\n+-------------+----------+------+-------------------+--------------+-----------+--------------------------+\n|person_BRONZE|person    |BRONZE|2026-03-01 07:24:00|1000000       |SUCCESS    |2026-03-01 07:49:13.787503|\n|person_SILVER|person    |SILVER|NULL               |0             |INITIALIZED|2026-03-01 07:49:02.018525|\n|person_GOLD  |person    |GOLD  |NULL               |0             |INITIALIZED|2026-03-01 07:49:02.018525|\n|person_DIM   |person    |DIM   |NULL               |0             |INITIALIZED|2026-03-01 07:49:02.018525|\n+-------------+----------+------+-------------------+--------------+-----------+--------------------------+\n\n================================================================================\n‚úÖ‚úÖ‚úÖ PHASE 1 - STEP 3 COMPLETE ‚úÖ‚úÖ‚úÖ\n================================================================================\n\nüìã STEP 3 SUMMARY:\n================================================================================\nPHASE 1 - STEP 3: ETL CONTROL TABLE\n================================================================================\nStatus: ‚úÖ COMPLETE\n\n‚úÖ CREATED:\n   Table: dbo.etl_control\n   Records: 4\n   Tracked Tables: 1\n   Tracked Layers: 4\n\n‚úÖ INITIALIZED:\n   person: BRONZE, SILVER, GOLD, DIM\n\n‚úÖ HELPER FUNCTIONS:\n   - get_last_watermark(table_name, layer)\n   - update_watermark(...)\n   - get_watermark_history(...)\n   - get_tables_to_process()\n\n‚úÖ TESTED:\n   - Watermark retrieval: ‚úÖ\n   - Watermark update: ‚úÖ\n   - History query: ‚úÖ\n   - Table listing: ‚úÖ\n================================================================================\n\nüìå NEXT STEPS:\n1. ‚úÖ Step 1: Audit columns added\n2. ‚úÖ Step 2: Synthetic generator updated\n3. ‚úÖ Step 3: ETL control table created\n4. ‚è≠Ô∏è  Step 4: Implement incremental load in ETL v4.0\n\nüéØ READY FOR STEP 4:\n   - Control table ready: ‚úÖ\n   - Watermarks initialized: ‚úÖ\n   - Helper functions available: ‚úÖ\n   - Test data with timestamps: ‚úÖ (1M records from Step 2)\n\n‚è≠Ô∏è  Next: Modify ETL v3.2 ‚Üí v4.0 for incremental load\n   Expected result: 469 seconds ‚Üí ~5 seconds (99% faster)\n================================================================================\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"37df1a03-9ae4-4a83-a1c2-7e1739e341f9"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"5d277b6d-26ce-4249-a7d9-a6eac8face79"}],"default_lakehouse":"5d277b6d-26ce-4249-a7d9-a6eac8face79","default_lakehouse_name":"Lake24","default_lakehouse_workspace_id":"591f29e6-d45a-4989-9459-a5a7bf1b39b8"}}},"nbformat":4,"nbformat_minor":5}